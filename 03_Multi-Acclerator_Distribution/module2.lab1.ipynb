{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95e9c9ba",
      "metadata": {},
      "source": [
        "# Module 2\n",
        "## Lab 1: Multi-Accelerator Distribtuion\n",
        "In this lab we will discuss how we can leverage multiple accelerators in a single device to perform distributed operations. We will review previous concepts, re-contextualized with multiple GPUs.\n",
        "\n",
        "### Pre-Requisite Knowledge\n",
        "We highly suggestion you read up on [sharding concepts and collective communications](https://jax-ml.github.io/scaling-book/sharding/).\n",
        "\n",
        "### In this Lab You Will:\n",
        "- Run a larger version of llama on multiple GPUs\n",
        "- Use different sharding methods\n",
        "- Do a basic distributed GEMM Calculation \n",
        "- Understand the different parallization approaches\n",
        "- (Optional) See a more detailed/conceptual breakdown of how matrices are sharded across GPUs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a07b6a95",
      "metadata": {},
      "source": [
        "### Imports and GPU Information\n",
        "Here we import the relevant libraries and retrieve detailed information about the available GPUs in our environment. We use pynvml to get low-level GPU metrics (e.g., name, memory size, clock speeds), and set up any necessary environment variables (like PyTorch memory allocations). This step helps us understand and confirm our hardware configuration. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "da81d30e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "os.chdir(parent_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ee4db012",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: deepspeed==0.16.2 in /home/ec2-user/.local/lib/python3.12/site-packages (0.16.2)\n",
            "Requirement already satisfied: transformers==4.47.1 in /home/ec2-user/.local/lib/python3.12/site-packages (4.47.1)\n",
            "Requirement already satisfied: accelerate==1.2.1 in /home/ec2-user/.local/lib/python3.12/site-packages (1.2.1)\n",
            "Requirement already satisfied: torch in /home/ec2-user/.local/lib/python3.12/site-packages (2.6.0)\n",
            "Requirement already satisfied: pynvml in /home/ec2-user/.local/lib/python3.12/site-packages (12.0.0)\n",
            "Requirement already satisfied: matplotlib in /home/ec2-user/.local/lib/python3.12/site-packages (3.10.1)\n",
            "Requirement already satisfied: numpy in /home/ec2-user/.local/lib/python3.12/site-packages (2.2.4)\n",
            "Requirement already satisfied: scipy in /home/ec2-user/.local/lib/python3.12/site-packages (1.15.2)\n",
            "Requirement already satisfied: torchvision in /home/ec2-user/.local/lib/python3.12/site-packages (0.21.0)\n",
            "Requirement already satisfied: mpi4py in /home/ec2-user/.local/lib/python3.12/site-packages (4.0.3)\n",
            "Requirement already satisfied: einops in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (0.8.1)\n",
            "Requirement already satisfied: hjson in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (1.1.0)\n",
            "Requirement already satisfied: ninja in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (1.11.1.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (24.2)\n",
            "Requirement already satisfied: psutil in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (7.0.0)\n",
            "Requirement already satisfied: py-cpuinfo in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (2.11.4)\n",
            "Requirement already satisfied: tqdm in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (12.570.86)\n",
            "Requirement already satisfied: filelock in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (80.0.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed==0.16.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed==0.16.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed==0.16.2) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (2025.1.31)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Environment Setup\n",
        "%pip install deepspeed==0.16.2 transformers==4.47.1 accelerate==1.2.1 torch pynvml matplotlib numpy scipy torchvision mpi4py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "87efa90e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available GPUs: 4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 2040 MHz\n",
            "  Memory Clock: 6251 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "GPU 1: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 210 MHz\n",
            "  Memory Clock: 405 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "GPU 2: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 210 MHz\n",
            "  Memory Clock: 405 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "GPU 3: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 210 MHz\n",
            "  Memory Clock: 405 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "\n",
            "Note: For detailed CUDA core and tensor core counts, refer to NVIDIA official GPU specifications.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Imports and GPU Information\n",
        "\n",
        "import torch\n",
        "import pynvml\n",
        "import os\n",
        "\n",
        "# Configure PyTorch memory\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "pynvml.nvmlInit()\n",
        "num_gpus = pynvml.nvmlDeviceGetCount()\n",
        "memory_bandwidth_tb_s = 0.3\n",
        "\n",
        "print(f\"Available GPUs: {num_gpus}\")\n",
        "\n",
        "gpu_info = {}\n",
        "for i in range(num_gpus):\n",
        "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "    name = pynvml.nvmlDeviceGetName(handle)\n",
        "    memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "    mem_total_tb = memory_info.total / (1024 ** 3)\n",
        "    clock_info = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_GRAPHICS)\n",
        "    mem_clock_info = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_MEM)\n",
        "    compute_capability = torch.cuda.get_device_capability(i)\n",
        "    \n",
        "    gpu_info[i] = {\n",
        "        'name': name,\n",
        "        'memory_tb': mem_total_tb,\n",
        "        'gpu_clock_mhz': clock_info,\n",
        "        'mem_clock_mhz': mem_clock_info,\n",
        "        'memory_bandwidth_tb_s': memory_bandwidth_tb_s,\n",
        "        'compute_capability': compute_capability\n",
        "    }\n",
        "    \n",
        "    print(f\"GPU {i}: {name}\")\n",
        "    print(f\"  Memory: {mem_total_tb:.2f} GB\")\n",
        "    print(f\"  GPU Clock: {clock_info} MHz\")\n",
        "    print(f\"  Memory Clock: {mem_clock_info} MHz\")\n",
        "    print(f\"  Approx. Memory Bandwidth: {memory_bandwidth_tb_s:.2f} TB/s\")\n",
        "    print(f\"  Compute Capability: {compute_capability}\")\n",
        "\n",
        "pynvml.nvmlShutdown()\n",
        "print(\"\\nNote: For detailed CUDA core and tensor core counts, refer to NVIDIA official GPU specifications.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "953dd823",
      "metadata": {},
      "source": [
        "**This time we will use all 4 GPUs**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3b8a99e",
      "metadata": {},
      "source": [
        "### Our Baseline\n",
        "\n",
        "Like last time, let's aim to deploy our llama model. This time we'll try to get closer to the larger model we want to run. So we'll do a llama model at 7B parameters. We'll still try to run it on one GPU.\n",
        "\n",
        "We'll use the same abstraction we used last time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "13d7dd42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-01 14:31:21,696] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/bin/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n",
            "/usr/bin/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîÅ Running batch size = 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2209f9626746418aa1a85c0b2aebb281",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 30.81 MiB is free. Process 572146 has 184.00 MiB memory in use. Including non-PyTorch memory, this process has 21.82 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 18.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmutils\u001b[39;00m\n\u001b[32m      7\u001b[39m importlib.reload(mutils)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m results = \u001b[43mmutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbenchmark_batch_sizes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNousResearch/Nous-Hermes-Llama2-13b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(results, indent=\u001b[32m2\u001b[39m))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/environment/src/utils/model_utils.py:47\u001b[39m, in \u001b[36mbenchmark_batch_sizes\u001b[39m\u001b[34m(model_name, seq_len, min_new_tokens, batch_sizes, dtype, sharding, world_size, rank)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müîÅ Running batch size = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     elapsed_s, tokens_generated, metrics, cost = \u001b[43mbenchmark_llm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m     results.append({\n\u001b[32m     58\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: batch,\n\u001b[32m     59\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mworld_size\u001b[39m\u001b[33m\"\u001b[39m: world_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcost_per_1m_tokens\u001b[39m\u001b[33m\"\u001b[39m: cost,\n\u001b[32m     63\u001b[39m     })\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# Cleanup between runs\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/environment/src/utils/model_utils.py:184\u001b[39m, in \u001b[36mbenchmark_llm\u001b[39m\u001b[34m(model_name, batch, seq_len, min_new_tokens, dtype, sharding, world_size)\u001b[39m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Load tokenizer and model\u001b[39;00m\n\u001b[32m    183\u001b[39m tok = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m model = \u001b[43mload_sharded_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m model.config.use_cache = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    186\u001b[39m torch.backends.cuda.matmul.allow_tf32 = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/environment/src/utils/model_utils.py:101\u001b[39m, in \u001b[36mload_sharded_model\u001b[39m\u001b[34m(model_name, dtype, sharding, world_size)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Single-GPU, no parallelism\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sharding == \u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     98\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.eval()\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# DataParallel: replicate model, shard input\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m sharding == \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/transformers/modeling_utils.py:3164\u001b[39m, in \u001b[36mPreTrainedModel.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3159\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[32m   3160\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3161\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3162\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3163\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1343\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1340\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1341\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "    \u001b[31m[... skipping similar frames: Module._apply at line 903 (2 times)]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:903\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    907\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    908\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    913\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    914\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:930\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    927\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    931\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1329\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1324\u001b[39m             device,\n\u001b[32m   1325\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1326\u001b[39m             non_blocking,\n\u001b[32m   1327\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1328\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1329\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 30.81 MiB is free. Process 572146 has 184.00 MiB memory in use. Including non-PyTorch memory, this process has 21.82 GiB memory in use. Of the allocated memory 21.62 GiB is allocated by PyTorch, and 18.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pynvml\n",
        "import os\n",
        "import json\n",
        "import importlib\n",
        "import src.utils.model_utils as mutils\n",
        "importlib.reload(mutils)\n",
        "\n",
        "results = mutils.benchmark_batch_sizes(\n",
        "    model_name=\"NousResearch/Nous-Hermes-Llama2-13b\",\n",
        "    seq_len=32,\n",
        "    min_new_tokens=1,\n",
        "    batch_sizes=[1],\n",
        "    dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "print(json.dumps(results, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7eb4a29",
      "metadata": {},
      "source": [
        "We ran out of memory! This is to be expected this model is more than 13x the size of the last model we ran. Batching won't solve this, now we'll have to split hte model up by utilizing sharding.\n",
        "\n",
        "Let's clear our GPU memory first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1d24ce82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-05 20:43:05,934] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/bin/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n",
            "/usr/bin/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'importlib' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmutils\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mimportlib\u001b[49m.reload(mutils)\n\u001b[32m      4\u001b[39m gc.collect()\n\u001b[32m      6\u001b[39m mutils.reset_distributed_and_clear_memory()\n",
            "\u001b[31mNameError\u001b[39m: name 'importlib' is not defined"
          ]
        }
      ],
      "source": [
        "import src.utils.model_utils as mutils\n",
        "import gc\n",
        "importlib.reload(mutils)\n",
        "gc.collect()\n",
        "\n",
        "mutils.reset_distributed_and_clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa9d227e",
      "metadata": {},
      "source": [
        "Now let's run our model 2 GPUs instead by utilizing sharding. This way we essentially double our memory, and split the model (parameters) across both GPUs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4a64f492",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-05 20:43:13,569] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-05-05 20:43:13,592] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "üîÅ Running batch size = 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  7.86it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  7.79it/s]\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-05 20:43:16,597] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
            "[2025-05-05 20:43:16,597] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2025-05-05 20:43:16,598] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "[2025-05-05 20:43:16,600] [INFO] [comm.py:652:init_distributed] cdb=None\n",
            "[2025-05-05 20:43:16,655] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 5120, 'intermediate_size': 13824, 'heads': 40, 'num_hidden_layers': -1, 'dtype': torch.bfloat16, 'pre_layer_norm': True, 'norm_type': <NormType.RMSNorm: 3>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 2, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 128, 'rotate_half': True, 'rotate_every_two': False, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GATED_SILU: 4>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 33, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000.0, 'invert_mask': True}\n",
            "[2025-05-05 20:43:16,714] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
            "[2025-05-05 20:43:16,714] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2025-05-05 20:43:16,714] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "[2025-05-05 20:43:16,717] [INFO] [comm.py:652:init_distributed] cdb=None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
            "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/transformer_inference/build.ninja...\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module transformer_inference...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module transformer_inference...\n",
            "Loading extension module transformer_inference...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ninja: no work to do.\n",
            "Time to load transformer_inference op: 0.09843659400939941 seconds\n",
            "Time to load transformer_inference op: 0.10500359535217285 seconds\n",
            "------------------------------------------------------\n",
            "Free memory : 7.072205 (GigaBytes)  \n",
            "Total memory: 22.045044 (GigaBytes)  \n",
            "Requested memory: 0.015776 (GigaBytes) \n",
            "Setting maximum total tokens (input + output) to 33 \n",
            "WorkSpace: 0x7fafcb400000 \n",
            "------------------------------------------------------\n",
            "Batch=1 | Seq=32+1\n",
            "Elapsed GPU time: 0.0657s | TFLOP/s: 6.7 | AI: 33.00 FLOP/B\n",
            "üß† Tokens generated: 1\n",
            "‚ö° Throughput: 15.21244 tokens/sec\n",
            "‚è±Ô∏è Total time: 0.06574 sec\n",
            "üí∏ Cost per 1M tokens: $22.09449\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
            "    sem_unlink(name)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
            "    sem_unlink(name)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"batch_size\": 1,\n",
            "    \"world_size\": 2,\n",
            "    \"avg_time_seconds\": 0.06573567962646484,\n",
            "    \"local_gflops\": 6699.136263629782,\n",
            "    \"aggregated_gflops\": 6699.136263629782,\n",
            "    \"total_flops\": 440372275200,\n",
            "    \"estimated_memory_bytes\": 13345280000,\n",
            "    \"arithmetic_intensity\": 32.998354114713216,\n",
            "    \"cost_per_1m_tokens\": 22.094492318895124\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import pynvml\n",
        "import os\n",
        "import json\n",
        "import importlib\n",
        "import src.utils.model_utils as mutils\n",
        "importlib.reload(mutils)\n",
        "\n",
        "results = mutils.run_distributed_benchmark(\n",
        "    model_name=\"NousResearch/Nous-Hermes-Llama2-13b\",\n",
        "    seq_len=32,\n",
        "    min_new_tokens=1,\n",
        "    batch_sizes=[1],\n",
        "    dtype=torch.bfloat16,\n",
        "    sharding=\"tensor\",\n",
        "    world_size=2, # Number of GPUs\n",
        ")\n",
        "\n",
        "print(json.dumps(results, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dc68d6a",
      "metadata": {},
      "source": [
        "It takes about a minute on these GPUs to load the model into memory (one time). But once it finishes we can fit a much larger model into memory by utilizing mulitiple GPUs. And based on our last lab we can likely improve this by increasing the batch size as well.\n",
        "\n",
        "So what's happening?\n",
        "- We're splitting the model's weights across GPUs (sharding or tensor parallism), allowing us to fit a model with many more weights than we could on a single GPU\n",
        "- We are introducing communication between these GPUs, or **collective communications**\n",
        "\n",
        "> Note the library we're using in this case **Deepspeed** is likely doing additional optimizations\n",
        "\n",
        "Next let's peel away the library and see what's happening under the hood with sharding, or splitting data across GPUs, and collectivs where the GPUs communicate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75856362",
      "metadata": {},
      "source": [
        "## Utilizing Multiple GPUs with GEMM\n",
        "\n",
        "In this section we'll demonstrate how data is split across GPUs, and how communication is achieved. This is a very deep topic so we'll only be covering the surface so you understand what the libraries you're utilizing are doing unde the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0128d87a",
      "metadata": {},
      "source": [
        "### In Practice\n",
        "Let's go back to our GEMM calculation. This time let's use a much larger GEMM operation, and split it across our GPUs. First let's try to run a 26 GB matrix multiplication on a single GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "78ce39df",
      "metadata": {},
      "outputs": [
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 25.71 GiB. GPU 0 has a total capacity of 22.05 GiB of which 21.86 GiB is free. Including non-PyTorch memory, this process has 184.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Define matrix dimensions (e.g., 200_000 x 200_000 of float32 ~= 149 GB)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# We'll cut this down to fit ~26 GB (e.g., 115_000 x 60_000 float32)\u001b[39;00m\n\u001b[32m      5\u001b[39m rows, cols = \u001b[32m115_000\u001b[39m, \u001b[32m60_000\u001b[39m  \u001b[38;5;66;03m# ~26 GB total\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m A = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda:0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Matrix to multiply with (on each GPU, send it there)\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Shape: [cols, 1024] -> Output shape will be [rows, 1024]\u001b[39;00m\n\u001b[32m     11\u001b[39m B = torch.randn((cols, \u001b[32m1024\u001b[39m), dtype=torch.float32, device=\u001b[33m'\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 25.71 GiB. GPU 0 has a total capacity of 22.05 GiB of which 21.86 GiB is free. Including non-PyTorch memory, this process has 184.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Define matrix dimensions (e.g., 200_000 x 200_000 of float32 ~= 149 GB)\n",
        "# We'll cut this down to fit ~26 GB (e.g., 115_000 x 60_000 float32)\n",
        "rows, cols = 115_000, 60_000  # ~26 GB total\n",
        "\n",
        "A = torch.randn((rows, cols), dtype=torch.float32, device='cuda:0')\n",
        "\n",
        "# Matrix to multiply with (on each GPU, send it there)\n",
        "# Shape: [cols, 1024] -> Output shape will be [rows, 1024]\n",
        "B = torch.randn((cols, 1024), dtype=torch.float32, device='cuda:0')\n",
        "\n",
        "out = A @ B\n",
        "\n",
        "print(f\"Output shape: {out.shape}\")  # [115000, 1024]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e4bfb2",
      "metadata": {},
      "source": [
        "Predictably this failed. Now let's manually device the matrix in half, and move each half to a seperate GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5ac63f5e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([115000, 1024])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Assumes 2 GPUs available\n",
        "assert torch.cuda.device_count() >= 2\n",
        "\n",
        "# Simulate a large matrix that won't fit on one GPU\n",
        "# We'll split it along the row dimension\n",
        "half_rows = rows // 2\n",
        "\n",
        "# Allocate on GPU 0\n",
        "A0 = torch.randn((half_rows, cols), dtype=torch.float32, device='cuda:0')\n",
        "\n",
        "# Allocate on GPU 1\n",
        "A1 = torch.randn((rows - half_rows, cols), dtype=torch.float32, device='cuda:1')\n",
        "\n",
        "# Matrix to multiply with (on each GPU, send it there)\n",
        "# Shape: [cols, 1024] -> Output shape will be [rows, 1024]\n",
        "B = torch.randn((cols, 1024), dtype=torch.float32)\n",
        "\n",
        "# Send appropriate B chunks to GPUs\n",
        "B0 = B.to('cuda:0')\n",
        "B1 = B.to('cuda:1')\n",
        "\n",
        "# Multiply independently on both GPUs\n",
        "with torch.no_grad():\n",
        "    out0 = A0 @ B0\n",
        "    out1 = A1 @ B1\n",
        "\n",
        "# Bring result back to CPU\n",
        "out = torch.cat([out0.cpu(), out1.cpu()], dim=0)\n",
        "\n",
        "print(f\"Output shape: {out.shape}\")  # [115000, 1024]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe2d38d",
      "metadata": {},
      "source": [
        "Great! As you can see we were able to process a massive matrix across 2 GPUs easily, by simply providing half of the data to one GPU and half to the other. You can think of this like splitting your weights across 2 GPUs, that is effectively what libraries like Deepspeed are doing. \n",
        "\n",
        "This works great for 1 calculation, but what happens when you need to use the output of this for the next calculation? This is how neural networks and transformers work, one output is used as the input for the next layer. In the case we did above we just write back to the CPU, but this can be slow, so we want to keep data on GPU and have these GPUs communicate.\n",
        "\n",
        "This is where collectives come into play."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae0564f",
      "metadata": {},
      "source": [
        "#### Collectives\n",
        "\n",
        "Here we will take our matrices, split them across GPUs, and communicate these changes across those GPUs so they could perform the next calculation without going back to the CPU. \n",
        "\n",
        "We'll effectively be doing the same thing, but adding an \"all_gather\" step. This informs the GPUs to communicate the results of the GEMM to each other GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "76d2eb07",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Rank 0] Starting process on cuda:0\n",
            "[Rank 1] Starting process on cuda:1\n",
            "[Rank 1] Initialized NCCL process group with world_size=2\n",
            "[Rank 0] Initialized NCCL process group with world_size=2[Rank 1] Created local shard of A: shape=torch.Size([57500, 60000]) on cuda:1\n",
            "\n",
            "[Rank 1] Allocated empty matrix B to receive broadcast: shape=torch.Size([60000, 1024]) on cuda:1\n",
            "[Rank 0] Created local shard of A: shape=torch.Size([57500, 60000]) on cuda:0\n",
            "[Rank 0] Created full matrix B: shape=torch.Size([60000, 1024]) on cuda:0\n",
            "[Rank 0] Completed broadcast of B\n",
            "[Rank 0] Performing matmul: A_local (torch.Size([57500, 60000])) @ B (torch.Size([60000, 1024]))\n",
            "[Rank 1] Completed broadcast of B\n",
            "[Rank 1] Performing matmul: A_local (torch.Size([57500, 60000])) @ B (torch.Size([60000, 1024]))\n",
            "[Rank 0] Finished matmul. Output shape: torch.Size([57500, 1024]). Time: 0.697s\n",
            "[Rank 0] Prepared buffers for all_gather\n",
            "[Rank 0] Completed all_gather of local outputs\n",
            "[Rank 1] Finished matmul. Output shape: torch.Size([57500, 1024]). Time: 0.716s\n",
            "[Rank 1] Prepared buffers for all_gather\n",
            "[Rank 1] Completed all_gather of local outputs\n",
            "[Rank 1] Full output assembled: shape=torch.Size([115000, 1024]) (still on cuda:1)\n",
            "[Rank 0] Full output assembled: shape=torch.Size([115000, 1024]) (still on cuda:0)\n",
            "[Rank 0] Saved result to shared_results\n",
            "[Rank 0] Destroyed process group and exiting\n",
            "[Rank 1] Destroyed process group and exiting\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'output_shape': torch.Size([115000, 1024]),\n",
              "  'rows_per_rank': 57500,\n",
              "  'device': 'cuda:0'}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import importlib\n",
        "import src.utils.gemm_utils as gutils\n",
        "importlib.reload(gutils)\n",
        "\n",
        "gutils.distributed_gemm()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f265866",
      "metadata": {},
      "source": [
        "If you read through the results you should see the following:\n",
        "\n",
        "Two processes are launched, one per GPU (cuda:0 and cuda:1). Each process initializes its own NCCL (GPU communications) communication context as part of a world of 2 ranks.\n",
        "Both ranks independently allocate a shard of the large matrix A, each of shape [57500, 60000], representing half of the full input.\n",
        "\n",
        "Rank 0 generates the shared weight matrix B with shape [60000, 1024], while Rank 1 allocates an empty buffer for B. Rank 0 then broadcasts B to Rank 1 so that both GPUs have the same weights.\n",
        "\n",
        "Each rank performs matrix multiplication using its local A shard and the full B, producing an output tensor of shape [57500, 1024]. These operations take approximately 0.7 seconds on each GPU.\n",
        "\n",
        "After local matmul, both ranks allocate output buffers and perform an all_gather, collecting the outputs from each rank. This results in a fully assembled output tensor of shape [115000, 1024] on both GPUs.\n",
        "\n",
        "Finally, Rank 0 logs the output metadata, and both ranks cleanly shut down their distributed process groups.\n",
        "\n",
        "It is highly recomended you read through [gemm_utils.py](../src/utils/gemm_utils.py), specifically the `_distributed_gemm_worker` function to gain a full understanding of what each GPU is running. This is effectively what pytorch, deepspeed, and most other libraries are using under the hood to break up and aggregate results from matrix multiplications.\n",
        "\n",
        "> Note in practice you won't be writing these from scratch, however having an understanding of what these libraries are doing provide you capabilities to optimize your workload to a very deep level once you hit massive scale"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bbf94e6",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this portion of the lab we learned how you can utilize sharding and collectives to allow your GPUs to collaborate. In the next portion of this lab, we will learn how common libraries utilize this technique at a high level to get optimal performance, and the different strategies that can be employed."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
