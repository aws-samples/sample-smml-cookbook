{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2\n",
    "## Lab 2: Back to Libraries\n",
    "\n",
    "Now that we understand HOW sharding is done, we can understand how this tool is utilized by  libaries to optimize your model inference/training across multiple accelerators. If you want a deeper dive on individual sharding patterns that make up parallism patterns, and the collectives associated with them, we highly recommend [this chapter](https://jax-ml.github.io/scaling-book/sharding/) of **How to Scale your Model**.\n",
    "\n",
    "In this notebook though, we will focus on the different parallism strategies that can be implemented by these libaries, and what the considerations with them are.\n",
    "\n",
    "### Parallism Patterns\n",
    "This lab will be a hands on representation of common parallism patterns. If you want a achemic deep dive, we suggest you read through [The Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high_level_overview).\n",
    "\n",
    "> Note we will be using Deepspeed here but this could just as easily be done with any distributed inference/training framework, they have similar interfaces\n",
    "\n",
    "We will be covering the following:\n",
    "- Data Parallism\n",
    "- Tensor Parallism (Like we covered last chapter)\n",
    "- Pipeline Parallism\n",
    "- ZeRO\n",
    "\n",
    "There are other strategies as well, and likely more will emerge, but these are the main ones and others will follow similar concepts. \n",
    "\n",
    "Let's start by installing our libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Parallel\n",
    "**How it works:**\n",
    "Each GPU has a full copy of the model. Batches are split across GPUs. Gradients are synced after the backward pass.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "- Easy to implement (e.g., torch.nn.DataParallel, DDP)\n",
    "- Scales well for small to mid-sized models\n",
    "- No model code changes required\n",
    "- Best for large data sets and small models\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "- Inefficient for very large models (can't fit on one GPU)\n",
    "- All-reduce on gradients becomes a bottleneck at high scale\n",
    "\n",
    "\n",
    "Let's see it in action. We'll use a smaller model (1B) because we aren't splitting the model this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 20:59:09,466] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "üîÅ Running batch size = 10\n",
      "[\n",
      "  {\n",
      "    \"batch_size\": 10,\n",
      "    \"world_size\": 1,\n",
      "    \"avg_time_seconds\": 0.025810943603515626,\n",
      "    \"local_gflops\": 40218.75805650925,\n",
      "    \"aggregated_gflops\": 40218.75805650925,\n",
      "    \"total_flops\": 1038084096000,\n",
      "    \"estimated_memory_bytes\": 2474659840,\n",
      "    \"arithmetic_intensity\": 419.4855709946786,\n",
      "    \"cost_per_1m_tokens\": 0.08675344933403863\n",
      "  }\n",
      "]\n",
      "[2025-05-05 20:59:16,813] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-05 20:59:16,844] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "üîÅ Running batch size = 10\n",
      "[\n",
      "  {\n",
      "    \"batch_size\": 10,\n",
      "    \"world_size\": 2,\n",
      "    \"avg_time_seconds\": 0.6220800170898437,\n",
      "    \"local_gflops\": 41003.100108126644,\n",
      "    \"aggregated_gflops\": 41003.100108126644,\n",
      "    \"total_flops\": 25507209216000,\n",
      "    \"estimated_memory_bytes\": 2515210240,\n",
      "    \"arithmetic_intensity\": 10141.18375090585,\n",
      "    \"cost_per_1m_tokens\": 0.020908800574408633\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "os.environ[\"DEEPSPEED_LOG_LEVEL\"] = \"error\"\n",
    "\n",
    "# Create a temporary deepspeed config file\n",
    "ds_config = {\n",
    "    \"train_batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"fp16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\"stage\": 0}\n",
    "}\n",
    "\n",
    "# Running with 1 GPU\n",
    "result = mutils.run_deepspeed_inference(model_name=\"NousResearch/Llama-3.2-1B\", ds_config=ds_config, world_size=1)\n",
    "mutils.reset_distributed_and_clear_memory()\n",
    "print(result)\n",
    "\n",
    "# Copy the model to 2 GPUs and split the data\n",
    "mutils.run_deepspeed_inference(model_name=\"NousResearch/Llama-3.2-1B\", ds_config=ds_config)\n",
    "mutils.reset_distributed_and_clear_memory()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As you can see distributed data can allow us to process our workload much faster, this is similar to how we previously used batching to improove price/performance. Here we can do something similar. But once our model is large enough this stragegy will no longer work, and we'll run into the same issues as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Distributed env torn down and memory cleared.\n"
     ]
    }
   ],
   "source": [
    "mutils.reset_distributed_and_clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Parallism\n",
    "**How it works:**\n",
    "Individual layers are sharded across GPUs ‚Äî e.g., split matrix rows/columns in linear layers.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "- Enables sharding of very large models/layers\n",
    "- Reduces per-GPU memory usage\n",
    "- Exploits fine-grained parallelism within layers\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "- Requires deep model rewrites or tools like DeepSpeed/FSDP\n",
    "- Requires custom communication (e.g., all_gather, reduce_scatter)\n",
    "- Collective comms (e.g., NCCL) can dominate runtime if not optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-05 20:49:57,427] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-05 20:49:57,436] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "üîÅ Running batch size = 1\n",
      "[\n",
      "  {\n",
      "    \"batch_size\": 1,\n",
      "    \"world_size\": 2,\n",
      "    \"avg_time_seconds\": 0.0659137954711914,\n",
      "    \"local_gflops\": 6681.033493094344,\n",
      "    \"aggregated_gflops\": 6681.033493094344,\n",
      "    \"total_flops\": 440372275200,\n",
      "    \"estimated_memory_bytes\": 13345280000,\n",
      "    \"arithmetic_intensity\": 32.998354114713216,\n",
      "    \"cost_per_1m_tokens\": 22.154359033372664\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Nous-Hermes-Llama2-13b\",\n",
    "    seq_len=32,\n",
    "    min_new_tokens=1,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=\"tensor\",\n",
    "    world_size=2, # Number of GPUs\n",
    ")\n",
    "\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Parallelism\n",
    "**How it works:**\n",
    "Each GPU holds a different stage of the model. Batches are split into micro-batches and passed between GPUs sequentially.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "Works well for extremely deep models\n",
    "- Spreads compute and memory across GPUs\n",
    "- Compatible with tensor parallelism for hybrid scaling\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "- Latency due to pipeline bubbles (idle GPUs while others compute)\n",
    "- Complex micro-batching & scheduling\n",
    "- Harder to load balance if layers are uneven in cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
