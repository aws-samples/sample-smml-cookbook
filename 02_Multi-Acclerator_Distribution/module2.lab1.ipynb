{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95e9c9ba",
      "metadata": {},
      "source": [
        "# Module 2\n",
        "## Lab 1: Multi-Accelerator Distribtuion\n",
        "In this lab we will discuss how we can leverage multiple accelerators in a single device to perform distributed operations. We will review previous concepts, re-contextualized with multiple GPUs.\n",
        "\n",
        "### Pre-Requisite Knowledge\n",
        "We highly suggestion you read up on [sharding concepts and collective communications](https://jax-ml.github.io/scaling-book/sharding/).\n",
        "\n",
        "### In this Lab You Will:\n",
        "- Run a larger version of llama on multiple GPUs\n",
        "- Use different sharding methods\n",
        "- Do a basic distributed GEMM Calculation \n",
        "- Understand the different parallization approaches\n",
        "- (Optional) See a more detailed/conceptual breakdown of how matrices are sharded across GPUs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a07b6a95",
      "metadata": {},
      "source": [
        "### Imports and GPU Information\n",
        "Here we import the relevant libraries and retrieve detailed information about the available GPUs in our environment. We use pynvml to get low-level GPU metrics (e.g., name, memory size, clock speeds), and set up any necessary environment variables (like PyTorch memory allocations). This step helps us understand and confirm our hardware configuration. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "da81d30e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "os.chdir(parent_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ee4db012",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: deepspeed==0.16.2 in /home/ec2-user/.local/lib/python3.12/site-packages (0.16.2)\n",
            "Requirement already satisfied: transformers==4.47.1 in /home/ec2-user/.local/lib/python3.12/site-packages (4.47.1)\n",
            "Requirement already satisfied: accelerate==1.2.1 in /home/ec2-user/.local/lib/python3.12/site-packages (1.2.1)\n",
            "Requirement already satisfied: torch in /home/ec2-user/.local/lib/python3.12/site-packages (2.6.0)\n",
            "Requirement already satisfied: pynvml in /home/ec2-user/.local/lib/python3.12/site-packages (12.0.0)\n",
            "Requirement already satisfied: matplotlib in /home/ec2-user/.local/lib/python3.12/site-packages (3.10.1)\n",
            "Requirement already satisfied: numpy in /home/ec2-user/.local/lib/python3.12/site-packages (2.2.4)\n",
            "Requirement already satisfied: scipy in /home/ec2-user/.local/lib/python3.12/site-packages (1.15.2)\n",
            "Requirement already satisfied: torchvision in /home/ec2-user/.local/lib/python3.12/site-packages (0.21.0)\n",
            "Requirement already satisfied: mpi4py in /home/ec2-user/.local/lib/python3.12/site-packages (4.0.3)\n",
            "Requirement already satisfied: einops in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (0.8.1)\n",
            "Requirement already satisfied: hjson in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (1.1.0)\n",
            "Requirement already satisfied: ninja in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (1.11.1.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (24.2)\n",
            "Requirement already satisfied: psutil in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (7.0.0)\n",
            "Requirement already satisfied: py-cpuinfo in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (2.11.4)\n",
            "Requirement already satisfied: tqdm in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (12.570.86)\n",
            "Requirement already satisfied: filelock in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (80.3.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed==0.16.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed==0.16.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed==0.16.2) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (2025.1.31)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Environment Setup\n",
        "%pip install deepspeed==0.16.2 transformers==4.47.1 accelerate==1.2.1 torch pynvml matplotlib numpy scipy torchvision mpi4py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "87efa90e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available GPUs: 4\n",
            "GPU 0: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 2040 MHz\n",
            "  Memory Clock: 6251 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "GPU 1: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 330 MHz\n",
            "  Memory Clock: 405 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "GPU 2: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 1260 MHz\n",
            "  Memory Clock: 6251 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "GPU 3: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 1020 MHz\n",
            "  Memory Clock: 6251 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "\n",
            "Note: For detailed CUDA core and tensor core counts, refer to NVIDIA official GPU specifications.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Imports and GPU Information\n",
        "\n",
        "import torch\n",
        "import pynvml\n",
        "import os\n",
        "\n",
        "# Configure PyTorch memory\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "pynvml.nvmlInit()\n",
        "num_gpus = pynvml.nvmlDeviceGetCount()\n",
        "memory_bandwidth_tb_s = 0.3\n",
        "\n",
        "print(f\"Available GPUs: {num_gpus}\")\n",
        "\n",
        "gpu_info = {}\n",
        "for i in range(num_gpus):\n",
        "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "    name = pynvml.nvmlDeviceGetName(handle)\n",
        "    memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "    mem_total_tb = memory_info.total / (1024 ** 3)\n",
        "    clock_info = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_GRAPHICS)\n",
        "    mem_clock_info = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_MEM)\n",
        "    compute_capability = torch.cuda.get_device_capability(i)\n",
        "    \n",
        "    gpu_info[i] = {\n",
        "        'name': name,\n",
        "        'memory_tb': mem_total_tb,\n",
        "        'gpu_clock_mhz': clock_info,\n",
        "        'mem_clock_mhz': mem_clock_info,\n",
        "        'memory_bandwidth_tb_s': memory_bandwidth_tb_s,\n",
        "        'compute_capability': compute_capability\n",
        "    }\n",
        "    \n",
        "    print(f\"GPU {i}: {name}\")\n",
        "    print(f\"  Memory: {mem_total_tb:.2f} GB\")\n",
        "    print(f\"  GPU Clock: {clock_info} MHz\")\n",
        "    print(f\"  Memory Clock: {mem_clock_info} MHz\")\n",
        "    print(f\"  Approx. Memory Bandwidth: {memory_bandwidth_tb_s:.2f} TB/s\")\n",
        "    print(f\"  Compute Capability: {compute_capability}\")\n",
        "\n",
        "pynvml.nvmlShutdown()\n",
        "print(\"\\nNote: For detailed CUDA core and tensor core counts, refer to NVIDIA official GPU specifications.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "953dd823",
      "metadata": {},
      "source": [
        "**This time we will use all 4 GPUs**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3b8a99e",
      "metadata": {},
      "source": [
        "### Our Baseline\n",
        "\n",
        "Like last time, let's aim to deploy our llama model. This time we'll try to get closer to the larger model we want to run. So we'll do a llama model at 7B parameters. We'll still try to run it on one GPU.\n",
        "\n",
        "We'll use the same abstraction we used last time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "13d7dd42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-08 15:26:46,085] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/bin/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n",
            "/usr/bin/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71adb945495a47d38332fa3982032a5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load failed: CUDA out of memory. Tried to allocate 136.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 79.94 MiB is free. Including non-PyTorch memory, this process has 21.96 GiB memory in use. Of the allocated memory 21.77 GiB is allocated by PyTorch, and 8.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pynvml\n",
        "import os\n",
        "import json\n",
        "import importlib\n",
        "import src.utils.model_utils as mutils\n",
        "importlib.reload(mutils)\n",
        "\n",
        "results = mutils.benchmark_batch_sizes(\n",
        "    model_name=\"NousResearch/Nous-Hermes-Llama2-13b\",\n",
        "    seq_len=32,\n",
        "    min_new_tokens=1,\n",
        "    batch_sizes=[1],\n",
        "    dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7eb4a29",
      "metadata": {},
      "source": [
        "We ran out of memory! This is to be expected this model is more than 13x the size of the last model we ran. Batching won't solve this, now we'll have to split hte model up by utilizing sharding.\n",
        "\n",
        "Let's clear our GPU memory first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1d24ce82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Distributed env torn down and memory cleared.\n"
          ]
        }
      ],
      "source": [
        "import src.utils.model_utils as mutils\n",
        "import gc\n",
        "importlib.reload(mutils)\n",
        "gc.collect()\n",
        "import psutil\n",
        "import torch.distributed as dist\n",
        "\n",
        "mutils.reset_distributed_and_clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa9d227e",
      "metadata": {},
      "source": [
        "Now let's run our model 2 GPUs instead by utilizing sharding. This way we essentially double our memory, and split the model (parameters) across both GPUs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4a64f492",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-08 15:26:56,317] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-05-08 15:26:56,349] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.90it/s]\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00,  7.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-08 15:26:59,255] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
            "[2025-05-08 15:26:59,255] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2025-05-08 15:26:59,255] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "[2025-05-08 15:26:59,258] [INFO] [comm.py:652:init_distributed] cdb=None\n",
            "ninja: no work to do.\n",
            "Time to load transformer_inference op: 0.02422809600830078 seconds\n",
            "[2025-05-08 15:26:59,345] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
            "[2025-05-08 15:26:59,345] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
            "[2025-05-08 15:26:59,345] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
            "[2025-05-08 15:26:59,348] [INFO] [comm.py:652:init_distributed] cdb=None\n",
            "[2025-05-08 15:26:59,388] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 5120, 'intermediate_size': 13824, 'heads': 40, 'num_hidden_layers': -1, 'dtype': torch.bfloat16, 'pre_layer_norm': True, 'norm_type': <NormType.RMSNorm: 3>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 2, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 128, 'rotate_half': True, 'rotate_every_two': False, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GATED_SILU: 4>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 33, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000.0, 'invert_mask': True}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/transformer_inference/build.ninja...\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module transformer_inference...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module transformer_inference...\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/transformer_inference/build.ninja...\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Building extension module transformer_inference...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module transformer_inference...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ninja: no work to do.\n",
            "Time to load transformer_inference op: 0.02243328094482422 seconds\n",
            "\n",
            "ðŸ” Running batch size = 1\n",
            "------------------------------------------------------\n",
            "Free memory : 8.856262 (GigaBytes)  \n",
            "Total memory: 22.045044 (GigaBytes)  \n",
            "Requested memory: 0.015776 (GigaBytes) \n",
            "Setting maximum total tokens (input + output) to 33 \n",
            "WorkSpace: 0x7f6a85400000 \n",
            "------------------------------------------------------\n",
            "Batch=1 | Seq=32+1\n",
            "Elapsed GPU time: 0.0659s | TFLOP/s: 6.7 | AI: 33.00 FLOP/B\n",
            "--------- OUTPUT BREAKDOWN ---------\n",
            "ðŸ§  Tokens generated: 2\n",
            "âš¡ Throughput: 30.31055 tokens/sec\n",
            "â±ï¸ Total time: 0.06598 sec\n",
            "ðŸ’¸ Cost per 1M tokens: $11.08891\n",
            "------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
            "    sem_unlink(name)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
            "    sem_unlink(name)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import importlib\n",
        "import torch\n",
        "import src.utils.model_utils as mutils\n",
        "importlib.reload(mutils)\n",
        "\n",
        "seq_len = 32\n",
        "min_new_tokens = 1\n",
        "world_size = 2\n",
        "max_tokens =  seq_len + min_new_tokens\n",
        "\n",
        "ds_config = {\n",
        "    \"replace_with_kernel_inject\": True,\n",
        "    \"enable_cuda_graph\": False,\n",
        "    \"tensor_parallel\": {\n",
        "        \"enabled\": True,\n",
        "        \"tp_size\": world_size\n",
        "    },\n",
        "    # Optional tuning knobs to constrain token planning\n",
        "    \"max_tokens\": max_tokens\n",
        "}\n",
        "\n",
        "results = mutils.run_distributed_benchmark(\n",
        "    model_name=\"NousResearch/Nous-Hermes-Llama2-13b\",\n",
        "    seq_len=32,\n",
        "    min_new_tokens=1,\n",
        "    batch_sizes=[1],\n",
        "    dtype=torch.bfloat16,\n",
        "    sharding=True,\n",
        "    world_size=2, # Number of GPUs\n",
        "    ds_config=ds_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dc68d6a",
      "metadata": {},
      "source": [
        "It takes about a minute on these GPUs to load the model into memory (one time). But once it finishes we can fit a much larger model into memory by utilizing mulitiple GPUs. And based on our last lab we can likely improve this by increasing the batch size as well.\n",
        "\n",
        "So what's happening?\n",
        "- We're splitting the model's weights across GPUs (sharding or tensor parallism), allowing us to fit a model with many more weights than we could on a single GPU\n",
        "- We are introducing communication between these GPUs, or **collective communications**\n",
        "\n",
        "> Note the library we're using in this case **Deepspeed** is likely doing additional optimizations\n",
        "\n",
        "Next let's peel away the library and see what's happening under the hood with sharding, or splitting data across GPUs, and collectivs where the GPUs communicate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75856362",
      "metadata": {},
      "source": [
        "## Utilizing Multiple GPUs with GEMM\n",
        "\n",
        "In this section we'll demonstrate how data is split across GPUs, and how communication is achieved. This is a very deep topic so we'll only be covering the surface so you understand what the libraries you're utilizing are doing unde the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0128d87a",
      "metadata": {},
      "source": [
        "### In Practice\n",
        "Let's go back to our GEMM calculation. This time let's use a much larger GEMM operation, and split it across our GPUs. First let's try to run a 26 GB matrix multiplication on a single GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "78ce39df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load failed: CUDA out of memory. Tried to allocate 25.71 GiB. GPU 0 has a total capacity of 22.05 GiB of which 21.86 GiB is free. Including non-PyTorch memory, this process has 184.00 MiB memory in use. Of the allocated memory 0 bytes is allocated by PyTorch, and 0 bytes is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "try:\n",
        "    # Define matrix dimensions (e.g., 200_000 x 200_000 of float32 ~= 149 GB)\n",
        "    # We'll cut this down to fit ~26 GB (e.g., 115_000 x 60_000 float32)\n",
        "    rows, cols = 115_000, 60_000  # ~26 GB total\n",
        "\n",
        "    A = torch.randn((rows, cols), dtype=torch.float32, device='cuda:0')\n",
        "\n",
        "    # Matrix to multiply with (on each GPU, send it there)\n",
        "    # Shape: [cols, 1024] -> Output shape will be [rows, 1024]\n",
        "    B = torch.randn((cols, 1024), dtype=torch.float32, device='cuda:0')\n",
        "\n",
        "    out = A @ B\n",
        "except Exception as e:\n",
        "    print(f\"load failed: {e}\")\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e4bfb2",
      "metadata": {},
      "source": [
        "Predictably this failed. Now let's manually device the matrix in half, and move each half to a seperate GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5ac63f5e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([115000, 1024])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import src.utils.model_utils as mutils\n",
        "import importlib\n",
        "import gc\n",
        "importlib.reload(mutils)\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "gc.collect()\n",
        "# Assumes 2 GPUs available\n",
        "assert torch.cuda.device_count() >= 2\n",
        "\n",
        "# Simulate a large matrix that won't fit on one GPU\n",
        "# We'll split it along the row dimension\n",
        "half_rows = rows // 2\n",
        "\n",
        "# Allocate on GPU 0\n",
        "A0 = torch.randn((half_rows, cols), dtype=torch.float32, device='cuda:0')\n",
        "\n",
        "# Allocate on GPU 1\n",
        "A1 = torch.randn((rows - half_rows, cols), dtype=torch.float32, device='cuda:1')\n",
        "\n",
        "# Matrix to multiply with (on each GPU, send it there)\n",
        "# Shape: [cols, 1024] -> Output shape will be [rows, 1024]\n",
        "B = torch.randn((cols, 1024), dtype=torch.float32)\n",
        "\n",
        "# Send appropriate B chunks to GPUs\n",
        "B0 = B.to('cuda:0')\n",
        "B1 = B.to('cuda:1')\n",
        "\n",
        "# Multiply independently on both GPUs\n",
        "with torch.no_grad():\n",
        "    out0 = A0 @ B0\n",
        "    out1 = A1 @ B1\n",
        "\n",
        "# Bring result back to CPU\n",
        "out = torch.cat([out0.cpu(), out1.cpu()], dim=0)\n",
        "\n",
        "del A0, A1, B0, B1, out0, out1\n",
        "\n",
        "print(f\"Output shape: {out.shape}\")  # [115000, 1024]\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe2d38d",
      "metadata": {},
      "source": [
        "Great! As you can see we were able to process a massive matrix across 2 GPUs easily, by simply providing half of the data to one GPU and half to the other. You can think of this like splitting your weights across 2 GPUs, that is effectively what libraries like Deepspeed are doing. \n",
        "\n",
        "This works great for 1 calculation, but what happens when you need to use the output of this for the next calculation? This is how neural networks and transformers work, one output is used as the input for the next layer. In the case we did above we just write back to the CPU, but this can be slow, so we want to keep data on GPU and have these GPUs communicate.\n",
        "\n",
        "This is where collectives come into play."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae0564f",
      "metadata": {},
      "source": [
        "#### Collectives\n",
        "\n",
        "Here we will take our matrices, split them across GPUs, and communicate these changes across those GPUs so they could perform the next calculation without going back to the CPU. \n",
        "\n",
        "We'll effectively be doing the same thing, but adding an \"all_gather\" step. This informs the GPUs to communicate the results of the GEMM to each other GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "76d2eb07",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Rank 0] Starting process on cuda:0\n",
            "[Rank 1] Starting process on cuda:1\n",
            "[Rank 1] Initialized NCCL process group with world_size=2\n",
            "[Rank 0] Initialized NCCL process group with world_size=2\n",
            "[Rank 1] Created local shard of A: shape=torch.Size([57500, 60000]) on cuda:1\n",
            "[Rank 1] Allocated empty matrix B to receive broadcast: shape=torch.Size([60000, 1024]) on cuda:1\n",
            "[Rank 0] Created local shard of A: shape=torch.Size([57500, 60000]) on cuda:0\n",
            "[Rank 0] Created full matrix B: shape=torch.Size([60000, 1024]) on cuda:0\n",
            "[Rank 0] Completed broadcast of B\n",
            "[Rank 0] Performing matmul: A_local (torch.Size([57500, 60000])) @ B (torch.Size([60000, 1024]))\n",
            "[Rank 1] Completed broadcast of B\n",
            "[Rank 1] Performing matmul: A_local (torch.Size([57500, 60000])) @ B (torch.Size([60000, 1024]))\n",
            "[Rank 0] Finished matmul. Output shape: torch.Size([57500, 1024]). Time: 0.682s\n",
            "[Rank 0] Prepared buffers for all_gather\n",
            "[Rank 0] Completed all_gather of local outputs\n",
            "[Rank 1] Finished matmul. Output shape: torch.Size([57500, 1024]). Time: 0.707s\n",
            "[Rank 1] Prepared buffers for all_gather\n",
            "[Rank 1] Completed all_gather of local outputs\n",
            "[Rank 1] Full output assembled: shape=torch.Size([115000, 1024]) (still on cuda:1)[Rank 0] Full output assembled: shape=torch.Size([115000, 1024]) (still on cuda:0)\n",
            "\n",
            "[Rank 0] Saved result to shared_results\n",
            "[Rank 0] Destroyed process group and exiting\n",
            "[Rank 1] Destroyed process group and exiting\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'output_shape': torch.Size([115000, 1024]),\n",
              "  'rows_per_rank': 57500,\n",
              "  'device': 'cuda:0'}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import importlib\n",
        "import src.utils.gemm_utils as gutils\n",
        "importlib.reload(gutils)\n",
        "\n",
        "gutils.distributed_gemm()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f265866",
      "metadata": {},
      "source": [
        "If you read through the results you should see the following:\n",
        "\n",
        "Two processes are launched, one per GPU (cuda:0 and cuda:1). Each process initializes its own NCCL (GPU communications) communication context as part of a world of 2 ranks.\n",
        "Both ranks independently allocate a shard of the large matrix A, each of shape [57500, 60000], representing half of the full input.\n",
        "\n",
        "Rank 0 generates the shared weight matrix B with shape [60000, 1024], while Rank 1 allocates an empty buffer for B. Rank 0 then broadcasts B to Rank 1 so that both GPUs have the same weights.\n",
        "\n",
        "Each rank performs matrix multiplication using its local A shard and the full B, producing an output tensor of shape [57500, 1024]. These operations take approximately 0.7 seconds on each GPU.\n",
        "\n",
        "After local matmul, both ranks allocate output buffers and perform an all_gather, collecting the outputs from each rank. This results in a fully assembled output tensor of shape [115000, 1024] on both GPUs.\n",
        "\n",
        "Finally, Rank 0 logs the output metadata, and both ranks cleanly shut down their distributed process groups.\n",
        "\n",
        "It is highly recomended you read through [gemm_utils.py](../src/utils/gemm_utils.py), specifically the `_distributed_gemm_worker` function to gain a full understanding of what each GPU is running. This is effectively what pytorch, deepspeed, and most other libraries are using under the hood to break up and aggregate results from matrix multiplications.\n",
        "\n",
        "> Note in practice you won't be writing these from scratch, however having an understanding of what these libraries are doing provide you capabilities to optimize your workload to a very deep level once you hit massive scale"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bbf94e6",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this portion of the lab we learned how you can utilize sharding and collectives to allow your GPUs to collaborate. In the next portion of this lab, we will learn how common libraries utilize this technique at a high level to get optimal performance, and the different strategies that can be employed."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
