{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "95e9c9ba",
      "metadata": {},
      "source": [
        "# Module 2\n",
        "## Lab 1: Multi-Accelerator Distribtuion\n",
        "In this lab we will discuss how we can leverage multiple accelerators in a single device to perform distributed operations. We will review previous concepts, re-contextualized with multiple GPUs.\n",
        "\n",
        "### Pre-Requisite Knowledge\n",
        "We highly suggestion you read up on [sharding concepts and collective communications](https://jax-ml.github.io/scaling-book/sharding/).\n",
        "\n",
        "### In this Lab You Will:\n",
        "- Run a larger version of llama on multiple GPUs\n",
        "- Use different sharding methods\n",
        "- Do a basic distributed GEMM Calculation \n",
        "- Understand the different parallization approaches\n",
        "- (Optional) See a more detailed/conceptual breakdown of how matrices are sharded across GPUs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a07b6a95",
      "metadata": {},
      "source": [
        "### Imports and GPU Information\n",
        "Here we import the relevant libraries and retrieve detailed information about the available GPUs in our environment. We use pynvml to get low-level GPU metrics (e.g., name, memory size, clock speeds), and set up any necessary environment variables (like PyTorch memory allocations). This step helps us understand and confirm our hardware configuration. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "da81d30e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
        "os.chdir(parent_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ee4db012",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: deepspeed==0.16.2 in /home/ec2-user/.local/lib/python3.12/site-packages (0.16.2)\n",
            "Requirement already satisfied: transformers==4.47.1 in /home/ec2-user/.local/lib/python3.12/site-packages (4.47.1)\n",
            "Requirement already satisfied: accelerate==1.2.1 in /home/ec2-user/.local/lib/python3.12/site-packages (1.2.1)\n",
            "Requirement already satisfied: torch in /home/ec2-user/.local/lib/python3.12/site-packages (2.6.0)\n",
            "Requirement already satisfied: pynvml in /home/ec2-user/.local/lib/python3.12/site-packages (12.0.0)\n",
            "Requirement already satisfied: matplotlib in /home/ec2-user/.local/lib/python3.12/site-packages (3.10.1)\n",
            "Requirement already satisfied: numpy in /home/ec2-user/.local/lib/python3.12/site-packages (2.2.4)\n",
            "Requirement already satisfied: scipy in /home/ec2-user/.local/lib/python3.12/site-packages (1.15.2)\n",
            "Requirement already satisfied: torchvision in /home/ec2-user/.local/lib/python3.12/site-packages (0.21.0)\n",
            "Requirement already satisfied: mpi4py in /home/ec2-user/.local/lib/python3.12/site-packages (4.0.3)\n",
            "Requirement already satisfied: einops in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (0.8.1)\n",
            "Requirement already satisfied: hjson in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (1.1.0)\n",
            "Requirement already satisfied: ninja in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (1.11.1.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (24.2)\n",
            "Requirement already satisfied: psutil in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (7.0.0)\n",
            "Requirement already satisfied: py-cpuinfo in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (2.11.4)\n",
            "Requirement already satisfied: tqdm in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed==0.16.2) (12.570.86)\n",
            "Requirement already satisfied: filelock in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers==4.47.1) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.47.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (80.7.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/.local/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed==0.16.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed==0.16.2) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed==0.16.2) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers==4.47.1) (2025.1.31)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Environment Setup\n",
        "%pip install deepspeed==0.16.2 transformers==4.47.1 accelerate==1.2.1 torch pynvml matplotlib numpy scipy torchvision mpi4py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "87efa90e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available GPUs: 4\n",
            "GPU 0: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 210 MHz\n",
            "  Memory Clock: 405 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "GPU 1: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 210 MHz\n",
            "  Memory Clock: 405 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "GPU 2: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 210 MHz\n",
            "  Memory Clock: 405 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "GPU 3: NVIDIA L4\n",
            "  Memory: 22.49 GB\n",
            "  GPU Clock: 210 MHz\n",
            "  Memory Clock: 405 MHz\n",
            "  Approx. Memory Bandwidth: 0.30 TB/s\n",
            "  Compute Capability: (8, 9)\n",
            "\n",
            "Note: For detailed CUDA core and tensor core counts, refer to NVIDIA official GPU specifications.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Imports and GPU Information\n",
        "\n",
        "import torch\n",
        "import pynvml\n",
        "import os\n",
        "\n",
        "# Configure PyTorch memory\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "pynvml.nvmlInit()\n",
        "num_gpus = pynvml.nvmlDeviceGetCount()\n",
        "memory_bandwidth_tb_s = 0.3\n",
        "\n",
        "print(f\"Available GPUs: {num_gpus}\")\n",
        "\n",
        "gpu_info = {}\n",
        "for i in range(num_gpus):\n",
        "    handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "    name = pynvml.nvmlDeviceGetName(handle)\n",
        "    memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "    mem_total_tb = memory_info.total / (1024 ** 3)\n",
        "    clock_info = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_GRAPHICS)\n",
        "    mem_clock_info = pynvml.nvmlDeviceGetClockInfo(handle, pynvml.NVML_CLOCK_MEM)\n",
        "    compute_capability = torch.cuda.get_device_capability(i)\n",
        "    \n",
        "    gpu_info[i] = {\n",
        "        'name': name,\n",
        "        'memory_tb': mem_total_tb,\n",
        "        'gpu_clock_mhz': clock_info,\n",
        "        'mem_clock_mhz': mem_clock_info,\n",
        "        'memory_bandwidth_tb_s': memory_bandwidth_tb_s,\n",
        "        'compute_capability': compute_capability\n",
        "    }\n",
        "    \n",
        "    print(f\"GPU {i}: {name}\")\n",
        "    print(f\"  Memory: {mem_total_tb:.2f} GB\")\n",
        "    print(f\"  GPU Clock: {clock_info} MHz\")\n",
        "    print(f\"  Memory Clock: {mem_clock_info} MHz\")\n",
        "    print(f\"  Approx. Memory Bandwidth: {memory_bandwidth_tb_s:.2f} TB/s\")\n",
        "    print(f\"  Compute Capability: {compute_capability}\")\n",
        "\n",
        "pynvml.nvmlShutdown()\n",
        "print(\"\\nNote: For detailed CUDA core and tensor core counts, refer to NVIDIA official GPU specifications.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "953dd823",
      "metadata": {},
      "source": [
        "**This time we will use all 4 GPUs**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3b8a99e",
      "metadata": {},
      "source": [
        "### Our Baseline\n",
        "\n",
        "Like last time, let's aim to deploy our llama model. This time we'll try to get closer to the larger model we want to run. So we'll do a llama model at 7B parameters. We'll still try to run it on one GPU.\n",
        "\n",
        "We'll use the same abstraction we used last time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "13d7dd42",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-22 19:17:14,710] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/bin/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n",
            "/usr/bin/ld: cannot find -laio: No such file or directory\n",
            "collect2: error: ld returned 1 exit status\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6f1bc78d3de4dd6bff1971af3dc322e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîÅ Running batch size = 1\n",
            "benchmark failed: CUDA out of memory. Tried to allocate 112.00 MiB. GPU 0 has a total capacity of 22.05 GiB of which 51.94 MiB is free. Including non-PyTorch memory, this process has 21.99 GiB memory in use. Of the allocated memory 21.54 GiB is allocated by PyTorch, and 217.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pynvml\n",
        "import os\n",
        "import json\n",
        "import importlib\n",
        "import src.utils.model_utils as mutils\n",
        "importlib.reload(mutils)\n",
        "\n",
        "results = mutils.benchmark_batch_sizes(\n",
        "    model_name=\"NousResearch/Meta-Llama-3.1-8B\",\n",
        "    seq_len=32,\n",
        "    batch_sizes=[1],\n",
        "    dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7eb4a29",
      "metadata": {},
      "source": [
        "We ran out of memory! This is to be expected this model is more than 8x the size of the last model we ran. Batching won't solve this, now we'll have to split hte model up by utilizing sharding.\n",
        "\n",
        "Let's clear our GPU memory first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1d24ce82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Distributed env torn down and memory cleared.\n"
          ]
        }
      ],
      "source": [
        "import src.utils.model_utils as mutils\n",
        "import gc\n",
        "importlib.reload(mutils)\n",
        "gc.collect()\n",
        "import psutil\n",
        "import torch.distributed as dist\n",
        "\n",
        "mutils.reset_distributed_and_clear_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa9d227e",
      "metadata": {},
      "source": [
        "Now let's run our model 2 GPUs instead by utilizing sharding, data parallism, and other optimization techniques. This way we essentially double our memory, and split the model (parameters) across both GPUs. \n",
        "\n",
        "> Note we'll dive more into detail for these later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4a64f492",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-21 20:40:37,741] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-05-21 20:40:37,813] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-05-21 20:40:37,869] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-05-21 20:40:37,882] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
            "[2025-05-21 20:40:39,510] [INFO] [comm.py:652:init_distributed] cdb=None\n",
            "[2025-05-21 20:40:39,510] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "[2025-05-21 20:40:39,859] [INFO] [comm.py:652:init_distributed] cdb=None\n",
            "[2025-05-21 20:40:39,865] [INFO] [comm.py:652:init_distributed] cdb=None\n",
            "[2025-05-21 20:40:39,865] [INFO] [comm.py:652:init_distributed] cdb=None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  8.58it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  8.40it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  7.10it/s]\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00,  7.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-21 20:40:42,826] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
            "[2025-05-21 20:40:42,884] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
            "[2025-05-21 20:40:42,884] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized\n",
            "[2025-05-21 20:40:42,884] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
            "[2025-05-21 20:40:43,096] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
            "[2025-05-21 20:40:43,218] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
            "[2025-05-21 20:40:43,525] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
            "[2025-05-21 20:40:44,658] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
            "[2025-05-21 20:40:44,661] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
            "[2025-05-21 20:40:44,662] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
            "[2025-05-21 20:40:44,662] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
            "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
            "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
            "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
            "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
            "ninja: no work to do.\n",
            "Time to load cpu_adam op: 2.2883644104003906 seconds\n",
            "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
            "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
            "[2025-05-21 20:40:46,958] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
            "Time to load cpu_adam op: 2.374513626098633 seconds\n",
            "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
            "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
            "[2025-05-21 20:40:47,038] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2025-05-21 20:40:47,038] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
            "Time to load cpu_adam op: 2.3752243518829346 seconds\n",
            "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
            "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
            "Time to load cpu_adam op: 2.376797676086426 seconds\n",
            "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
            "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
            "[2025-05-21 20:40:47,044] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "[2025-05-21 20:40:47,045] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2025-05-21 20:40:47,045] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
            "[2025-05-21 20:40:47,045] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
            "[2025-05-21 20:40:47,049] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
            "[2025-05-21 20:40:47,050] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
            "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
            "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
            "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
            "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module cpu_adam...\n",
            "Loading extension module cpu_adam...\n",
            "Loading extension module cpu_adam...\n",
            "Loading extension module cpu_adam...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-05-21 20:40:47,172] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
            "[2025-05-21 20:40:47,173] [INFO] [utils.py:782:see_memory_usage] MA 14.96 GB         Max_MA 15.94 GB         CA 15.94 GB         Max_CA 16 GB \n",
            "[2025-05-21 20:40:47,173] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.09 GB, percent = 5.0%\n",
            "[2025-05-21 20:40:47,174] [INFO] [stage3.py:168:__init__] Reduce bucket size 500000000\n",
            "[2025-05-21 20:40:47,174] [INFO] [stage3.py:169:__init__] Prefetch bucket size 50000000\n",
            "[2025-05-21 20:40:47,284] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
            "[2025-05-21 20:40:47,285] [INFO] [utils.py:782:see_memory_usage] MA 14.96 GB         Max_MA 14.96 GB         CA 15.94 GB         Max_CA 16 GB \n",
            "[2025-05-21 20:40:47,285] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.09 GB, percent = 5.0%\n",
            "[2025-05-21 20:40:47,288] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
            "Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
            "[2025-05-21 20:40:53,987] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
            "[2025-05-21 20:40:53,987] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 14.96 GB         CA 15.94 GB         Max_CA 16 GB \n",
            "[2025-05-21 20:40:53,987] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 26.43 GB, percent = 14.5%\n",
            "[2025-05-21 20:40:54,103] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
            "[2025-05-21 20:40:54,104] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 15.94 GB         Max_CA 16 GB \n",
            "[2025-05-21 20:40:54,104] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 26.43 GB, percent = 14.5%\n",
            "[2025-05-21 20:41:00,501] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2\n",
            "[2025-05-21 20:41:00,523] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 15.94 GB         Max_CA 16 GB \n",
            "[2025-05-21 20:41:00,523] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 45.14 GB, percent = 24.8%\n",
            "[2025-05-21 20:41:00,889] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
            "[2025-05-21 20:41:00,911] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 15.94 GB         Max_CA 16 GB \n",
            "[2025-05-21 20:41:00,912] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 47.17 GB, percent = 26.0%\n",
            "[2025-05-21 20:41:02,856] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
            "[2025-05-21 20:41:02,857] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 15.94 GB         Max_CA 16 GB \n",
            "[2025-05-21 20:41:02,857] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 78.34 GB, percent = 43.1%\n",
            "[2025-05-21 20:41:03,138] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
            "[2025-05-21 20:41:03,139] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 15.94 GB         Max_CA 16 GB \n",
            "[2025-05-21 20:41:03,139] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 86.01 GB, percent = 47.3%\n",
            "[2025-05-21 20:41:10,912] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
            "[2025-05-21 20:41:10,913] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 15.94 GB         Max_CA 16 GB \n",
            "[2025-05-21 20:41:10,913] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 111.03 GB, percent = 61.1%\n",
            "[2025-05-21 20:41:10,913] [INFO] [stage3.py:528:_setup_for_real_optimizer] optimizer state initialized\n",
            "[2025-05-21 20:41:17,612] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2025-05-21 20:41:17,612] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 2.89 GB         CA 15.94 GB         Max_CA 16 GB \n",
            "[2025-05-21 20:41:17,612] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 129.64 GB, percent = 71.3%\n",
            "[2025-05-21 20:41:17,613] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\n",
            "[2025-05-21 20:41:17,613] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
            "[2025-05-21 20:41:17,613] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
            "[2025-05-21 20:41:17,613] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   amp_params ................... False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": \"autotuning_results\", \n",
            "    \"exps_dir\": \"autotuning_exps\", \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f68ac60a150>\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   dump_state ................... False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
            "[2025-05-21 20:41:17,614] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"recompute_fwd_factor\": 0.0, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
            "    \"enabled\": false, \n",
            "    \"persistent_storage_path\": null, \n",
            "    \"persistent_time_interval\": 100, \n",
            "    \"num_of_version_in_retention\": 2, \n",
            "    \"enable_nebula_load\": true, \n",
            "    \"load_path\": null\n",
            "}\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   optimizer_name ............... adamw\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   pld_params ................... False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   steps_per_print .............. None\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   train_batch_size ............. 4\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   world_size ................... 4\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
            "[2025-05-21 20:41:17,615] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
            "[2025-05-21 20:41:17,616] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
            "[2025-05-21 20:41:17,616] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
            "[2025-05-21 20:41:17,616] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\n",
            "[2025-05-21 20:41:17,616] [INFO] [config.py:989:print_user_config]   json = {\n",
            "    \"train_micro_batch_size_per_gpu\": 1, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"fp16\": {\n",
            "        \"enabled\": true\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 3, \n",
            "        \"offload_param\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"contiguous_gradients\": true, \n",
            "        \"overlap_comm\": true\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 3e-05, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-08, \n",
            "            \"weight_decay\": 0.01\n",
            "        }\n",
            "    }, \n",
            "    \"tensor_parallel\": {\n",
            "        \"enabled\": true, \n",
            "        \"tp_size\": 4\n",
            "    }, \n",
            "    \"replace_with_kernel_inject\": false, \n",
            "    \"enable_cuda_graph\": false\n",
            "}\n",
            "\n",
            "üîÅ Running batch size = 1\n",
            "[2025-05-21 20:41:34,896] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
            "[2025-05-21 20:41:34,896] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
            "[2025-05-21 20:41:51,207] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
            "[2025-05-21 20:41:51,208] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch=1 | SeqLen=32\n",
            "Elapsed GPU time: 14.0274s | TFLOP/s: 0.0 | AI: 22.85 FLOP/B\n",
            "Batch=1 | SeqLen=32\n",
            "Elapsed GPU time: 14.0298s | TFLOP/s: 0.0 | AI: 22.85 FLOP/B\n",
            "Batch=1 | SeqLen=32\n",
            "Elapsed GPU time: 14.0354s | TFLOP/s: 0.0 | AI: 22.85 FLOP/B\n",
            "Batch=1 | SeqLen=32\n",
            "Elapsed GPU time: 14.0382s | TFLOP/s: 0.0 | AI: 22.85 FLOP/B\n",
            "--------- OUTPUT BREAKDOWN ---------\n",
            "üß† Tokens generated: 128\n",
            "‚ö° Throughput: 9.12155 tokens/sec\n",
            "‚è±Ô∏è Total time: 14.03270 sec\n",
            "üí∏ Cost per 1M tokens: $36.84801\n",
            "------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
            "    sem_unlink(name)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
            "    sem_unlink(name)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
            "    sem_unlink(name)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
            "    finalizer()\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
            "    sem_unlink(name)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import importlib\n",
        "import torch\n",
        "import src.utils.model_utils as mutils\n",
        "importlib.reload(mutils)\n",
        "\n",
        "seq_len = 32\n",
        "min_new_tokens = 1\n",
        "world_size = 4\n",
        "\n",
        "ds_config = {\n",
        "    \"train_micro_batch_size_per_gpu\": 1,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "\n",
        "    \"fp16\": { \"enabled\": True },\n",
        "    \n",
        "    # Ignore this for now\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": True\n",
        "        },\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": True\n",
        "        },\n",
        "        # optional perf tweaks:\n",
        "        \"contiguous_gradients\": True,\n",
        "        \"overlap_comm\": True\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": 3e-5,\n",
        "            \"betas\": [0.9, 0.999],\n",
        "            \"eps\": 1e-8,\n",
        "            \"weight_decay\": 0.01\n",
        "        }\n",
        "    },\n",
        "    \n",
        "    # Splitting the model across 4 GPUs\n",
        "    \"tensor_parallel\": {\n",
        "        \"enabled\": True,\n",
        "        \"tp_size\": world_size\n",
        "    },\n",
        "\n",
        "    \"replace_with_kernel_inject\": False,\n",
        "    \"enable_cuda_graph\": False\n",
        "}\n",
        "\n",
        "\n",
        "results = mutils.run_distributed_benchmark(\n",
        "    model_name=\"NousResearch/Meta-Llama-3.1-8B\",\n",
        "    seq_len=32,\n",
        "    batch_sizes=[1],\n",
        "    dtype=torch.bfloat16,\n",
        "    sharding=True,\n",
        "    world_size=4, # Number of GPUs\n",
        "    ds_config=ds_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dc68d6a",
      "metadata": {},
      "source": [
        "It takes about a minute on these GPUs to load the model into memory (one time). But once it finishes we can fit a much larger model into memory by utilizing mulitiple GPUs. And based on our last lab we can likely improve this by increasing the batch size as well.\n",
        "\n",
        "So what's happening?\n",
        "- We're splitting the model's weights across GPUs (sharding or tensor parallism), allowing us to fit a model with many more weights than we could on a single GPU\n",
        "- We are introducing communication between these GPUs, or **collective communications**\n",
        "\n",
        "> Note the library we're using in this case **Deepspeed** is likely doing additional optimizations\n",
        "\n",
        "Next let's peel away the library and see what's happening under the hood with sharding, or splitting data across GPUs, and collectivs where the GPUs communicate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75856362",
      "metadata": {},
      "source": [
        "## Utilizing Multiple GPUs with GEMM\n",
        "\n",
        "In this section we'll demonstrate how data is split across GPUs, and how communication is achieved. This is a very deep topic so we'll only be covering the surface so you understand what the libraries you're utilizing are doing unde the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0128d87a",
      "metadata": {},
      "source": [
        "### In Practice\n",
        "Let's go back to our GEMM calculation. This time let's use a much larger GEMM operation, and split it across our GPUs. First let's try to run a 26 GB matrix multiplication on a single GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "78ce39df",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load failed: CUDA out of memory. Tried to allocate 25.71 GiB. GPU 0 has a total capacity of 22.05 GiB of which 21.79 GiB is free. Including non-PyTorch memory, this process has 250.00 MiB memory in use. Of the allocated memory 8.12 MiB is allocated by PyTorch, and 11.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "try:\n",
        "    # Define matrix dimensions (e.g., 200_000 x 200_000 of float32 ~= 149 GB)\n",
        "    # We'll cut this down to fit ~26 GB (e.g., 115_000 x 60_000 float32)\n",
        "    rows, cols = 115_000, 60_000  # ~26 GB total\n",
        "\n",
        "    A = torch.randn((rows, cols), dtype=torch.float32, device='cuda:0')\n",
        "\n",
        "    # Matrix to multiply with (on each GPU, send it there)\n",
        "    # Shape: [cols, 1024] -> Output shape will be [rows, 1024]\n",
        "    B = torch.randn((cols, 1024), dtype=torch.float32, device='cuda:0')\n",
        "\n",
        "    out = A @ B\n",
        "except Exception as e:\n",
        "    print(f\"load failed: {e}\")\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8e4bfb2",
      "metadata": {},
      "source": [
        "Predictably this failed. Now let's manually device the matrix in half, and move each half to a seperate GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5ac63f5e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([115000, 1024])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import src.utils.model_utils as mutils\n",
        "import importlib\n",
        "import gc\n",
        "importlib.reload(mutils)\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "gc.collect()\n",
        "# Assumes 2 GPUs available\n",
        "assert torch.cuda.device_count() >= 2\n",
        "\n",
        "# Simulate a large matrix that won't fit on one GPU\n",
        "# We'll split it along the row dimension\n",
        "half_rows = rows // 2\n",
        "\n",
        "# Allocate on GPU 0\n",
        "A0 = torch.randn((half_rows, cols), dtype=torch.float32, device='cuda:0')\n",
        "\n",
        "# Allocate on GPU 1\n",
        "A1 = torch.randn((rows - half_rows, cols), dtype=torch.float32, device='cuda:1')\n",
        "\n",
        "# Matrix to multiply with (on each GPU, send it there)\n",
        "# Shape: [cols, 1024] -> Output shape will be [rows, 1024]\n",
        "B = torch.randn((cols, 1024), dtype=torch.float32)\n",
        "\n",
        "# Send appropriate B chunks to GPUs\n",
        "B0 = B.to('cuda:0')\n",
        "B1 = B.to('cuda:1')\n",
        "\n",
        "# Multiply independently on both GPUs\n",
        "with torch.no_grad():\n",
        "    out0 = A0 @ B0\n",
        "    out1 = A1 @ B1\n",
        "\n",
        "# Bring result back to CPU\n",
        "out = torch.cat([out0.cpu(), out1.cpu()], dim=0)\n",
        "\n",
        "del A0, A1, B0, B1, out0, out1\n",
        "\n",
        "print(f\"Output shape: {out.shape}\")  # [115000, 1024]\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe2d38d",
      "metadata": {},
      "source": [
        "Great! As you can see we were able to process a massive matrix across 2 GPUs easily, by simply providing half of the data to one GPU and half to the other. You can think of this like splitting your weights across 2 GPUs, that is effectively what libraries like Deepspeed are doing. \n",
        "\n",
        "This works great for 1 calculation, but what happens when you need to use the output of this for the next calculation? This is how neural networks and transformers work, one output is used as the input for the next layer. In the case we did above we just write back to the CPU, but this can be slow, so we want to keep data on GPU and have these GPUs communicate.\n",
        "\n",
        "This is where collectives come into play."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aae0564f",
      "metadata": {},
      "source": [
        "#### Collectives\n",
        "\n",
        "Here we will take our matrices, split them across GPUs, and communicate these changes across those GPUs so they could perform the next calculation without going back to the CPU. \n",
        "\n",
        "We'll effectively be doing the same thing, but adding an \"all_gather\" step. This informs the GPUs to communicate the results of the GEMM to each other GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "76d2eb07",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Rank 0] Starting process on cuda:0\n",
            "[Rank 1] Starting process on cuda:1\n",
            "[Rank 1] Initialized NCCL process group with world_size=2\n",
            "[Rank 0] Initialized NCCL process group with world_size=2\n",
            "[Rank 1] Created local shard of A: shape=torch.Size([57500, 60000]) on cuda:1\n",
            "[Rank 1] Allocated empty matrix B to receive broadcast: shape=torch.Size([60000, 1024]) on cuda:1\n",
            "[Rank 0] Created local shard of A: shape=torch.Size([57500, 60000]) on cuda:0\n",
            "[Rank 0] Created full matrix B: shape=torch.Size([60000, 1024]) on cuda:0\n",
            "[Rank 0] Completed broadcast of B\n",
            "[Rank 0] Performing matmul: A_local (torch.Size([57500, 60000])) @ B (torch.Size([60000, 1024]))\n",
            "[Rank 1] Completed broadcast of B\n",
            "[Rank 1] Performing matmul: A_local (torch.Size([57500, 60000])) @ B (torch.Size([60000, 1024]))\n",
            "[Rank 1] Finished matmul. Output shape: torch.Size([57500, 1024]). Time: 0.683s\n",
            "[Rank 1] Prepared buffers for all_gather\n",
            "[Rank 1] Completed all_gather of local outputs\n",
            "[Rank 0] Finished matmul. Output shape: torch.Size([57500, 1024]). Time: 0.698s\n",
            "[Rank 0] Prepared buffers for all_gather\n",
            "[Rank 0] Completed all_gather of local outputs\n",
            "[Rank 0] Full output assembled: shape=torch.Size([115000, 1024]) (still on cuda:0)\n",
            "[Rank 1] Full output assembled: shape=torch.Size([115000, 1024]) (still on cuda:1)\n",
            "[Rank 0] Saved result to shared_results\n",
            "[Rank 0] Destroyed process group and exiting\n",
            "[Rank 1] Destroyed process group and exiting\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'output_shape': torch.Size([115000, 1024]),\n",
              "  'rows_per_rank': 57500,\n",
              "  'device': 'cuda:0'}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import importlib\n",
        "import src.utils.gemm_utils as gutils\n",
        "importlib.reload(gutils)\n",
        "\n",
        "gutils.distributed_gemm()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f265866",
      "metadata": {},
      "source": [
        "If you read through the results you should see the following:\n",
        "\n",
        "Two processes are launched, one per GPU (cuda:0 and cuda:1). Each process initializes its own NCCL (GPU communications) communication context as part of a world of 2 ranks.\n",
        "Both ranks independently allocate a shard of the large matrix A, each of shape [57500, 60000], representing half of the full input.\n",
        "\n",
        "Rank 0 generates the shared weight matrix B with shape [60000, 1024], while Rank 1 allocates an empty buffer for B. Rank 0 then broadcasts B to Rank 1 so that both GPUs have the same weights.\n",
        "\n",
        "Each rank performs matrix multiplication using its local A shard and the full B, producing an output tensor of shape [57500, 1024]. These operations take approximately 0.7 seconds on each GPU.\n",
        "\n",
        "After local matmul, both ranks allocate output buffers and perform an all_gather, collecting the outputs from each rank. This results in a fully assembled output tensor of shape [115000, 1024] on both GPUs.\n",
        "\n",
        "Finally, Rank 0 logs the output metadata, and both ranks cleanly shut down their distributed process groups.\n",
        "\n",
        "It is highly recomended you read through [gemm_utils.py](../src/utils/gemm_utils.py), specifically the `_distributed_gemm_worker` function to gain a full understanding of what each GPU is running. This is effectively what pytorch, deepspeed, and most other libraries are using under the hood to break up and aggregate results from matrix multiplications.\n",
        "\n",
        "> Note in practice you won't be writing these from scratch, however having an understanding of what these libraries are doing provide you capabilities to optimize your workload to a very deep level once you hit massive scale"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bbf94e6",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this portion of the lab we learned how you can utilize sharding and collectives to allow your GPUs to collaborate. In the next portion of this lab, we will learn how common libraries utilize this technique at a high level to get optimal performance, and the different strategies that can be employed."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
