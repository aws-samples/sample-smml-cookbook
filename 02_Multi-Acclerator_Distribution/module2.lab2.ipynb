{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2\n",
    "## Lab 2: Back to Libraries\n",
    "\n",
    "Now that we understand HOW sharding is done, we can understand how this tool is utilized by  libaries to optimize your model inference/training across multiple accelerators. If you want a deeper dive on individual sharding patterns that make up parallism patterns, and the collectives associated with them, we highly recommend [this chapter](https://jax-ml.github.io/scaling-book/sharding/) of **How to Scale your Model**.\n",
    "\n",
    "In this notebook though, we will focus on the different parallism strategies that can be implemented by these libaries, and what the considerations with them are.\n",
    "\n",
    "### Parallism Patterns\n",
    "This lab will be a hands on representation of common parallism patterns. If you want a achemic deep dive, we suggest you read through [The Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high_level_overview).\n",
    "\n",
    "> Note we will be using Deepspeed here but this could just as easily be done with any distributed inference/training framework, they have similar interfaces\n",
    "\n",
    "We will be covering the following:\n",
    "- Data Parallism\n",
    "- Tensor Parallism (Like we covered last chapter)\n",
    "- Pipeline Parallism\n",
    "- ZeRO-3/FSDP (these can be interchangable in most contexts)\n",
    "\n",
    "There are other strategies as well, and likely more will emerge, but these are the main ones and others will follow similar concepts. Throughout the first part of this module we'll still be using a small model to demonstrate different types of parallism. At the end we will combine these strategies to run a much larger model on the same instance!\n",
    "\n",
    "The concepts we cover in this module can scale to thousands of GPUs.\n",
    "\n",
    "Let's start by installing our libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ec2-user/.local/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/.local/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: deepspeed in /home/ec2-user/.local/lib/python3.12/site-packages (0.16.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/.local/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: einops in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (0.8.1)\n",
      "Requirement already satisfied: hjson in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (1.1.0)\n",
      "Requirement already satisfied: ninja in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (1.11.1.4)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (2.11.4)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (12.570.86)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (80.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers accelerate deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run through the different types of parallism. Keep an eye out for `OUTPUT BREAKDOWN` to see what the token generation and cost looks like. You may see some warnings/errors but these are benign and can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "Before we get started we'll provide an appendix that describes the different images we'll be going through to visualize the types of parallism, their advantages and disadvantages.\n",
    "\n",
    "![](./assets/appendix.png)\n",
    "\n",
    "- The A Matrix represents data being input, as well as it's sequence length and batch size depending on how large it is\n",
    "- The B matrix represents the parameters for the model\n",
    "- The training states represent addition memory needed for forward/backward pass in training, in inference this isn't relevant, and you essentially only have the forward pass\n",
    "- We will be using GPUs but this could work with Neuron devices or any other accelerator\n",
    "- We won't be using 2x2 topologies, but in reality, that's how you'd model out your GPU topology when using multiple types of parallism\n",
    "\n",
    "With that out of the way let's jump into data parallism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Parallel\n",
    "![](./assets/dp.png)\n",
    "**How it works:**\n",
    "Each GPU has a full copy of the model. Batches are split across GPUs. Gradients are synced after the backward pass. As you can see this affectively just splits the input data. If your model is too large, or the training states take up too much memory this won't help reduce your memory footprint. It's best used as a tool to improve throughput.\n",
    "\n",
    "✅ Advantages:\n",
    "- Easy to implement (e.g., torch.nn.DataParallel, DDP)\n",
    "- Scales well for small to mid-sized models\n",
    "- No model code changes required\n",
    "- Best for large data sets and small models\n",
    "\n",
    "❌ Disadvantages:\n",
    "- Inefficient for very large models (can't fit on one GPU)\n",
    "- All-reduce on gradients becomes a bottleneck at high scale\n",
    "\n",
    "\n",
    "Let's see it in action. We'll use a smaller model (1B) because we aren't splitting the model this time.\n",
    "\n",
    "> Note: Deepspeed does this automatically as you provide basic optimization and multiple GPUs, as it's a standard optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 21:56:57,018] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:57:00,840] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "🔁 Running batch size = 4\n",
      "Batch=4|Seq=8 Elapsed=0.5190s TFLOPs=1.5 AI=22.86\n",
      "--- Breakdown ---\n",
      "Tokens: 32, Time: 0.519s, Cost/1M: $5.45082\n",
      "Cleared distributed env and caches.\n",
      "[2025-05-26 21:57:12,862] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:57:13,092] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:57:13,169] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:57:13,190] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:57:15,273] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 21:57:15,469] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 21:57:15,469] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-26 21:57:15,654] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 21:57:15,689] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 21:57:16,151] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:57:16,224] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:57:16,362] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:57:16,467] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-26 21:57:16,467] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized\n",
      "[2025-05-26 21:57:16,467] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:57:16,718] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.285951852798462 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-26 21:57:20,139] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-05-26 21:57:20,139] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-26 21:57:20,141] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2025-05-26 21:57:20,141] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2025-05-26 21:57:20,141] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2025-05-26 21:57:20,141] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 3 optimizer\n",
      "Time to load cpu_adam op: 2.375100612640381 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "Time to load cpu_adam op: 2.374450922012329 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "Time to load cpu_adam op: 2.375145673751831 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-26 21:57:20,234] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:57:20,234] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:57:20,235] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:57:20,262] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "[2025-05-26 21:57:20,263] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-26 21:57:20,263] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.54 GB, percent = 4.2%\n",
      "[2025-05-26 21:57:20,264] [INFO] [stage3.py:168:__init__] Reduce bucket size 500000000\n",
      "[2025-05-26 21:57:20,264] [INFO] [stage3.py:169:__init__] Prefetch bucket size 50000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 21:57:20,367] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-05-26 21:57:20,368] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-26 21:57:20,368] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.54 GB, percent = 4.2%\n",
      "[2025-05-26 21:57:20,369] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "Parameter Offload: Total persistent parameters: 67584 in 33 params\n",
      "[2025-05-26 21:57:22,768] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2025-05-26 21:57:22,769] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-26 21:57:22,769] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 10.25 GB, percent = 5.6%\n",
      "[2025-05-26 21:57:22,897] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
      "[2025-05-26 21:57:22,897] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-26 21:57:22,897] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 10.25 GB, percent = 5.6%\n",
      "[2025-05-26 21:57:25,573] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2025-05-26 21:57:25,574] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-26 21:57:25,574] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.8 GB, percent = 9.8%\n",
      "[2025-05-26 21:57:25,681] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
      "[2025-05-26 21:57:25,682] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-26 21:57:25,682] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.8 GB, percent = 9.8%\n",
      "[2025-05-26 21:57:25,908] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
      "[2025-05-26 21:57:25,908] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-26 21:57:25,908] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 18.96 GB, percent = 10.4%\n",
      "[2025-05-26 21:57:26,150] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-26 21:57:26,151] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-26 21:57:26,151] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.54 GB, percent = 14.0%\n",
      "[2025-05-26 21:57:27,809] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-26 21:57:27,809] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-26 21:57:27,810] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.12 GB, percent = 14.9%\n",
      "[2025-05-26 21:57:27,810] [INFO] [stage3.py:528:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2025-05-26 21:57:30,055] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-26 21:57:30,055] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 1.91 GB         CA 3.73 GB         Max_CA 4 GB \n",
      "[2025-05-26 21:57:30,056] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.48 GB, percent = 20.1%\n",
      "[2025-05-26 21:57:30,056] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\n",
      "[2025-05-26 21:57:30,056] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-26 21:57:30,056] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-26 21:57:30,056] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2025-05-26 21:57:30,056] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-05-26 21:57:30,056] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f08a0301610>\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   fp16_enabled ................. False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-26 21:57:30,057] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   optimizer_name ............... adamw\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   steps_per_print .............. None\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   train_batch_size ............. 4\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   world_size ................... 4\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\n",
      "[2025-05-26 21:57:30,058] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.01\n",
      "        }\n",
      "    }, \n",
      "    \"replace_with_kernel_inject\": false, \n",
      "    \"enable_cuda_graph\": false, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }\n",
      "}\n",
      "🔁 Running batch size = 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Benchmark failed: cannot reshape tensor of 0 elements into shape [0, 23, -1, 64] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "ERROR:root:Benchmark returned None\n",
      "ERROR:root:Benchmark failed: cannot reshape tensor of 0 elements into shape [0, 25, -1, 64] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "ERROR:root:Benchmark failed: cannot reshape tensor of 0 elements into shape [0, 26, -1, 64] because the unspecified dimension size -1 can be any value and is ambiguous\n",
      "ERROR:root:Benchmark failed: cannot reshape tensor of 0 elements into shape [0, 22, -1, 64] because the unspecified dimension size -1 can be any value and is ambiguous\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared distributed env and caches.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "# Generate a DeepSpeed config with key overrides only\n",
    "# Using batch size 2, grad accumulation 2, and no fp16\n",
    "ds_config = mutils.make_ds_config(\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "# Run single-GPU benchmark (no sharding)\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=8,\n",
    "    batch_sizes=[4],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=False,\n",
    "    world_size=1,\n",
    "    ds_config=ds_config\n",
    ")\n",
    "\n",
    "mutils.reset_distributed_and_clear_memory()\n",
    "\n",
    "# Run multi-GPU benchmark with sharding across 4 GPUs with a bigger batch size\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=32,\n",
    "    batch_sizes=[16],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=4,\n",
    "    ds_config=ds_config\n",
    ")\n",
    "\n",
    "mutils.reset_distributed_and_clear_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As you can see distributed data can allow us to process our workload much faster, this is similar to how we previously used batching to improove price/performance. Here we can do something similar. But once our model is large enough this stragegy will no longer work, and we'll run into the same issues as before. So we'll have to find ways to either reduce the model size or reduce the training states (in most cases both)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-3/FSDP\n",
    "![](./assets/zero.png)\n",
    "**How it works:**\n",
    "ZeRO-3 (and FSDP) shards all model states — including parameters, gradients, and optimizer states — across GPUs. It avoids redundancy by ensuring that no GPU holds a full copy of the model. Parameters are temporarily reconstructed using AllGather at compute time.\n",
    "\n",
    "✅ Advantages:\n",
    "- Shards the entire model, including non-linear layers, embeddings, and optimizer states\n",
    "- Enables training models that exceed per-GPU memory\n",
    "- Integrates with existing PyTorch models via FSDP or DeepSpeed ZeRO\n",
    "\n",
    "❌ Disadvantages:\n",
    "- Requires full parameter AllGather before each forward/backward step\n",
    "- Communication-intensive (especially with many GPUs)\n",
    "- Can be slower without high-bandwidth interconnect (e.g., NVLink or InfiniBand)\n",
    "\n",
    "> We will often stack ZeRO-3/FSDP with other strategies like Tensor and Data Parallelism. For example, we may shard linear layers with TP while still using ZeRO-3 to handle the remaining memory overhead from unsharded layers and optimizer state. This allows us to scale across both memory and compute bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 21:57:55,636] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:57:55,651] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:57:57,968] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 21:57:57,968] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-26 21:57:58,030] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 21:57:58,491] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-05-26 21:57:58,622] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-26 21:57:58,622] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized\n",
      "[2025-05-26 21:57:58,622] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-05-26 21:57:58,855] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-26 21:57:59,989] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "[2025-05-26 21:57:59,989] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combinationInstalled CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "\n",
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.2867889404296875 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-26 21:58:02,281] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Time to load cpu_adam op: 2.374570846557617 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-26 21:58:02,365] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-05-26 21:58:02,365] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-26 21:58:02,367] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2025-05-26 21:58:02,367] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2025-05-26 21:58:02,367] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2025-05-26 21:58:02,367] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 21:58:02,492] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "[2025-05-26 21:58:02,492] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.79 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:02,493] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 6.15 GB, percent = 3.4%\n",
      "[2025-05-26 21:58:02,493] [INFO] [stage3.py:168:__init__] Reduce bucket size 500000000\n",
      "[2025-05-26 21:58:02,493] [INFO] [stage3.py:169:__init__] Prefetch bucket size 50000000\n",
      "[2025-05-26 21:58:02,604] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-05-26 21:58:02,604] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:02,604] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 6.15 GB, percent = 3.4%\n",
      "[2025-05-26 21:58:02,606] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Parameter Offload: Total persistent parameters: 67584 in 33 params\n",
      "[2025-05-26 21:58:03,620] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2025-05-26 21:58:03,620] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 2.3 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:03,620] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.66 GB, percent = 4.8%\n",
      "[2025-05-26 21:58:03,730] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
      "[2025-05-26 21:58:03,731] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:03,731] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.66 GB, percent = 4.8%\n",
      "[2025-05-26 21:58:05,062] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2025-05-26 21:58:05,063] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:05,063] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 15.53 GB, percent = 8.5%\n",
      "[2025-05-26 21:58:05,181] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
      "[2025-05-26 21:58:05,182] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:05,182] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 15.05 GB, percent = 8.3%\n",
      "[2025-05-26 21:58:05,514] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
      "[2025-05-26 21:58:05,514] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:05,514] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.35 GB, percent = 9.5%\n",
      "[2025-05-26 21:58:05,694] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-26 21:58:05,694] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:05,695] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.01 GB, percent = 12.1%\n",
      "[2025-05-26 21:58:08,432] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-26 21:58:08,432] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:08,433] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.52 GB, percent = 14.0%\n",
      "[2025-05-26 21:58:08,433] [INFO] [stage3.py:528:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2025-05-26 21:58:10,608] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-26 21:58:10,609] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 1.91 GB         CA 3.73 GB         Max_CA 4 GB \n",
      "[2025-05-26 21:58:10,609] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 30.68 GB, percent = 16.9%\n",
      "[2025-05-26 21:58:10,609] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\n",
      "[2025-05-26 21:58:10,609] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-26 21:58:10,609] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-26 21:58:10,609] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2025-05-26 21:58:10,609] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fabf794ff50>\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "[2025-05-26 21:58:10,610] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   optimizer_name ............... adamw\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   steps_per_print .............. None\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   train_batch_size ............. 2\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\n",
      "[2025-05-26 21:58:10,611] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.01\n",
      "        }\n",
      "    }, \n",
      "    \"replace_with_kernel_inject\": false, \n",
      "    \"enable_cuda_graph\": false, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }\n",
      "}\n",
      "🔁 Running batch size = 1\n",
      "[2025-05-26 21:58:14,149] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
      "[2025-05-26 21:58:14,149] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "[2025-05-26 21:58:16,664] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2025-05-26 21:58:16,664] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "Batch=1|Seq=32 Elapsed=1.5907s TFLOPs=0.1 AI=22.85\n",
      "Batch=1|Seq=32 Elapsed=1.5955s TFLOPs=0.1 AI=22.85\n",
      "--- Breakdown ---\n",
      "Tokens: 64, Time: 1.593s, Cost/1M: $8.36663\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "seq_len = 32\n",
    "min_new_tokens = 1\n",
    "world_size = 2\n",
    "\n",
    "# Generate DeepSpeed config with FP16 enabled and default optimizer settings\n",
    "ds_config = mutils.make_ds_config()\n",
    "\n",
    "# Run benchmark with the generated config\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=seq_len,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=world_size,\n",
    "    ds_config=ds_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Parallism\n",
    "![](./assets/tp.png)\n",
    "**How it works:**\n",
    "Individual layers are sharded across GPUs — e.g., split matrix rows/columns in linear layers. Typically this requires a custom implementation of the model for parallism. This is usually done for popular models by frameworks like Pytorch and Deepspeed, but keep this in mind when using cutting edge models or creating your own, if the architecure is unique the model definition will need to account for this.\n",
    "\n",
    "✅ Advantages:\n",
    "- Enables sharding of very large models/layers\n",
    "- Reduces per-GPU memory usage\n",
    "- Exploits fine-grained parallelism within layers\n",
    "- Less CC than CC heavy DP\n",
    "- Reduces compute\n",
    "\n",
    "❌ Disadvantages:\n",
    "- Requires deep model rewrites or tools like DeepSpeed/FSDP\n",
    "- Requires custom communication (e.g., all_gather, reduce_scatter)\n",
    "- CC (e.g., NCCL) can dominate runtime if not optimized or scaled too high\n",
    "\n",
    "> We will stack types of parallism on top of each other as by themselves they may not be enough to to store in memory. For example, we will stack DP and TP in this case. You will see DP, and TP moving forward as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 21:58:25,266] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:58:25,324] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:58:25,349] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:58:25,364] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 21:58:27,582] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 21:58:27,582] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-26 21:58:27,856] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 21:58:27,929] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 21:58:27,934] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 21:58:28,412] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:58:28,461] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:58:28,535] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-26 21:58:28,535] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized\n",
      "[2025-05-26 21:58:28,535] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:58:28,595] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-26 21:58:28,852] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-26 21:58:29,987] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "[2025-05-26 21:58:29,987] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "[2025-05-26 21:58:29,987] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "[2025-05-26 21:58:29,988] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.2856407165527344 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-26 21:58:32,278] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "Time to load cpu_adam op: 2.3756699562072754 secondsTime to load cpu_adam op: 2.375784397125244 seconds\n",
      "\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-26 21:58:32,364] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-05-26 21:58:32,364] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-26 21:58:32,366] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2025-05-26 21:58:32,367] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2025-05-26 21:58:32,367] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2025-05-26 21:58:32,367] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2025-05-26 21:58:32,368] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "Time to load cpu_adam op: 2.3805012702941895 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-26 21:58:32,372] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 21:58:32,488] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "[2025-05-26 21:58:32,488] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.79 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:32,489] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.61 GB, percent = 4.2%\n",
      "[2025-05-26 21:58:32,489] [INFO] [stage3.py:168:__init__] Reduce bucket size 500000000\n",
      "[2025-05-26 21:58:32,489] [INFO] [stage3.py:169:__init__] Prefetch bucket size 50000000\n",
      "[2025-05-26 21:58:32,595] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-05-26 21:58:32,596] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.79 GB         Max_CA 3 GB \n",
      "[2025-05-26 21:58:32,596] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.61 GB, percent = 4.2%\n",
      "[2025-05-26 21:58:32,597] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     12\u001b[39m ds_config = mutils.make_ds_config(\n\u001b[32m     13\u001b[39m     tensor_parallel=world_size\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Run benchmark using tensor parallelism\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m results = \u001b[43mmutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_distributed_benchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNousResearch/Llama-3.2-1B\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mds_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_config\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/environment/sample-smml-cookbook/src/utils/model_utils.py:392\u001b[39m, in \u001b[36mrun_distributed_benchmark\u001b[39m\u001b[34m(model_name, seq_len, batch_sizes, dtype, sharding, world_size, ds_config)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Manager() \u001b[38;5;28;01mas\u001b[39;00m manager:\n\u001b[32m    391\u001b[39m     results = manager.list()\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m     \u001b[43mmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_distributed_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mds_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    397\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:340\u001b[39m, in \u001b[36mspawn\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method)\u001b[39m\n\u001b[32m    334\u001b[39m     msg = (\n\u001b[32m    335\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    336\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    337\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    338\u001b[39m     )\n\u001b[32m    339\u001b[39m     warnings.warn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspawn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:296\u001b[39m, in \u001b[36mstart_processes\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method)\u001b[39m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:144\u001b[39m, in \u001b[36mProcessContext.join\u001b[39m\u001b[34m(self, timeout, grace_period)\u001b[39m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Wait for any process to fail or all of them to succeed.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m ready = \u001b[43mmultiprocessing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msentinels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m error_index = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentinel \u001b[38;5;129;01min\u001b[39;00m ready:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    413\u001b[39m ready = []\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "seq_len = 32\n",
    "min_new_tokens = 1\n",
    "world_size = 4\n",
    "\n",
    "# Generate DeepSpeed config with FP16 and tensor parallelism across 4 GPUs\n",
    "ds_config = mutils.make_ds_config(\n",
    "    tensor_parallel=world_size\n",
    ")\n",
    "\n",
    "# Run benchmark using tensor parallelism\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=seq_len,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=world_size,\n",
    "    ds_config=ds_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Tensor parallism is exactly the same as what we demonstrated in the last lab. This allows us to launch a much larger model by utilizing more GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Parallelism\n",
    "![](./assets/pp.png)\n",
    "**How it works:**\n",
    "Each GPU holds a different stage of the model. Batches are split into micro-batches and passed between GPUs sequentially.\n",
    "\n",
    "✅ Advantages:\n",
    "Works well for extremely deep models\n",
    "- Spreads compute and memory across GPUs\n",
    "- Compatible with tensor parallelism for hybrid scaling\n",
    "- Reduce CC needed for parallism\n",
    "\n",
    "❌ Disadvantages:\n",
    "- Latency due to pipeline bubbles (idle GPUs while others compute)\n",
    "- Complex micro-batching & scheduling\n",
    "- Harder to load balance if layers are uneven in cost\n",
    "\n",
    "> Note: We will be using DP as deepspeed defaults, but no TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 22:21:42,139] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 22:21:42,182] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 22:21:42,199] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 22:21:42,206] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-26 22:21:45,181] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 22:21:45,181] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-26 22:21:45,451] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 22:21:45,458] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-26 22:21:45,458] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
      "Using topology: {ProcessCoord(pipe=0, data=0): 0, ProcessCoord(pipe=0, data=1): 1, ProcessCoord(pipe=1, data=0): 2, ProcessCoord(pipe=1, data=1): 3}\n",
      "[2025-05-26 22:21:45,620] [INFO] [module.py:396:_partition_layers] Partitioning pipeline stages with method parameters\n",
      "stage=0 layers=1\n",
      "     0: LlamaModel\n",
      "stage=1 layers=1\n",
      "     1: Linear\n",
      "  loss: CrossEntropyLoss\n",
      "[2025-05-26 22:21:45,632] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-05-26 22:21:45,640] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load fused_adam op: 0.040541648864746094 seconds\n",
      "[2025-05-26 22:21:45,967] [WARNING] [engine.py:1570:_configure_zero_optimizer] Pipeline parallelism does not support overlapped communication, will be disabled.\n",
      "Time to load fused_adam op: 0.10106611251831055 seconds\n",
      "[2025-05-26 22:21:45,973] [WARNING] [engine.py:1570:_configure_zero_optimizer] Pipeline parallelism does not support overlapped communication, will be disabled.\n",
      "[2025-05-26 22:21:45,981] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-05-26 22:21:46,113] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-26 22:21:46,113] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized\n",
      "[2025-05-26 22:21:46,113] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-05-26 22:21:46,320] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2025-05-26 22:21:46,326] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2025-05-26 22:21:46,339] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "ninja: no work to do.\n",
      "Time to load fused_adam op: 0.03986167907714844 seconds\n",
      "Time to load fused_adam op: 0.1011357307434082 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-26 22:21:46,854] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-05-26 22:21:46,854] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-26 22:21:46,856] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-05-26 22:21:46,856] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2025-05-26 22:21:46,856] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer\n",
      "[2025-05-26 22:21:46,856] [WARNING] [engine.py:1570:_configure_zero_optimizer] Pipeline parallelism does not support overlapped communication, will be disabled.\n",
      "[2025-05-26 22:21:46,856] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000\n",
      "[2025-05-26 22:21:46,856] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000\n",
      "[2025-05-26 22:21:46,856] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2025-05-26 22:21:46,856] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2025-05-26 22:21:46,856] [WARNING] [engine.py:1570:_configure_zero_optimizer] Pipeline parallelism does not support overlapped communication, will be disabled.\n",
      "[2025-05-26 22:21:48,812] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2025-05-26 22:21:48,856] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-26 22:21:48,856] [INFO] [utils.py:782:see_memory_usage] MA 4.61 GB         Max_MA 5.76 GB         CA 5.76 GB         Max_CA 6 GB \n",
      "[2025-05-26 22:21:48,874] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.05 GB, percent = 4.4%\n",
      "[2025-05-26 22:21:48,988] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-26 22:21:48,988] [INFO] [utils.py:782:see_memory_usage] MA 4.61 GB         Max_MA 6.91 GB         CA 8.06 GB         Max_CA 8 GB \n",
      "[2025-05-26 22:21:48,988] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.01 GB, percent = 4.4%\n",
      "[2025-05-26 22:21:48,988] [INFO] [stage_1_and_2.py:544:__init__] optimizer state initialized\n",
      "[2025-05-26 22:21:49,098] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-26 22:21:49,098] [INFO] [utils.py:782:see_memory_usage] MA 4.61 GB         Max_MA 4.61 GB         CA 8.06 GB         Max_CA 8 GB \n",
      "[2025-05-26 22:21:49,098] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.09 GB, percent = 4.4%\n",
      "[2025-05-26 22:21:49,099] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-05-26 22:21:49,099] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-26 22:21:49,099] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-26 22:21:49,100] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2025-05-26 22:21:49,100] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-05-26 22:21:49,100] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-26 22:21:49,100] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-26 22:21:49,100] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-05-26 22:21:49,100] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-05-26 22:21:49,100] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-26 22:21:49,100] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2025-05-26 22:21:49,100] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-05-26 22:21:49,100] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-26 22:21:49,100] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fa224362ed0>\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   optimizer_name ............... adamw\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 2, 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 1, 'pipe_partitioned': True, 'grad_partitioned': True, 'enabled': True, 'partition_method': 'parameters'}\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-05-26 22:21:49,101] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   steps_per_print .............. None\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   train_batch_size ............. 2\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 1\n",
      "[2025-05-26 22:21:49,102] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 1, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.01\n",
      "        }\n",
      "    }, \n",
      "    \"replace_with_kernel_inject\": false, \n",
      "    \"enable_cuda_graph\": false, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"tensor_parallel\": {\n",
      "        \"enabled\": true, \n",
      "        \"tp_size\": 2\n",
      "    }, \n",
      "    \"pipeline\": {\n",
      "        \"enabled\": true, \n",
      "        \"stages\": 2, \n",
      "        \"partition_method\": \"parameters\", \n",
      "        \"activation_checkpoint_interval\": 1\n",
      "    }\n",
      "}\n",
      "[2025-05-26 22:21:49,102] [INFO] [engine.py:105:__init__] CONFIG: micro_batches=1 micro_batch_size=1\n",
      "[2025-05-26 22:21:49,102] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2025-05-26 22:21:49,148] [INFO] [engine.py:165:__init__] RANK=0 STAGE=0 LAYERS=1 [0, 1) STAGE_PARAMS=1235814400 (1235.814M) TOTAL_PARAMS=1498482688 (1498.483M) UNIQUE_PARAMS=1498482688 (1498.483M)\n",
      "[2025-05-26 22:21:49,148] [INFO] [engine.py:165:__init__] RANK=2 STAGE=1 LAYERS=1 [1, 2) STAGE_PARAMS=262668288 (262.668M) TOTAL_PARAMS=1498482688 (1498.483M) UNIQUE_PARAMS=1498482688 (1498.483M)\n",
      "\n",
      "🔁 Running batch size = 1\n",
      "benchmark failed: \n",
      "benchmark failed: \n",
      "❌ Benchmark returned None; skipping.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "seq_len = 32\n",
    "world_size = 4\n",
    "\n",
    "# Generate DeepSpeed config with FP16, tensor parallelism, and pipeline parallelism across 4 GPUs\n",
    "ds_config = mutils.make_ds_config(\n",
    "    tensor_parallel=world_size //2,\n",
    "    pipeline={\n",
    "        \"stages\": world_size // 2,\n",
    "        \"partition_method\": \"parameters\",\n",
    "        \"activation_checkpoint_interval\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "# Run benchmark using tensor + pipeline parallelism\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=seq_len,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=world_size,\n",
    "    ds_config=ds_config\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
