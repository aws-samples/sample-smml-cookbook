{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2\n",
    "## Lab 2: Back to Libraries\n",
    "\n",
    "Now that we understand HOW sharding is done, we can understand how this tool is utilized by  libaries to optimize your model inference/training across multiple accelerators. If you want a deeper dive on individual sharding patterns that make up parallism patterns, and the collectives associated with them, we highly recommend [this chapter](https://jax-ml.github.io/scaling-book/sharding/) of **How to Scale your Model**.\n",
    "\n",
    "In this notebook though, we will focus on the different parallism strategies that can be implemented by these libaries, and what the considerations with them are.\n",
    "\n",
    "### Parallism Patterns\n",
    "This lab will be a hands on representation of common parallism patterns. If you want a achemic deep dive, we suggest you read through [The Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high_level_overview).\n",
    "\n",
    "> Note we will be using Deepspeed here but this could just as easily be done with any distributed inference/training framework, they have similar interfaces\n",
    "\n",
    "We will be covering the following:\n",
    "- Data Parallism\n",
    "- Tensor Parallism (Like we covered last chapter)\n",
    "- Pipeline Parallism\n",
    "- ZeRO\n",
    "\n",
    "There are other strategies as well, and likely more will emerge, but these are the main ones and others will follow similar concepts. \n",
    "\n",
    "Let's start by installing our libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ec2-user/.local/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/.local/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: deepspeed in /home/ec2-user/.local/lib/python3.12/site-packages (0.16.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/.local/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: einops in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (0.8.1)\n",
      "Requirement already satisfied: hjson in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (1.1.0)\n",
      "Requirement already satisfied: ninja in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (1.11.1.4)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (2.11.4)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (12.570.86)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (80.3.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers accelerate deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run through the different types of parallism. Keep an eye out for `OUTPUT BREAKDOWN` to see what the token generation and cost looks like. You may see some warnings/errors but these are benign and can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Parallel\n",
    "**How it works:**\n",
    "Each GPU has a full copy of the model. Batches are split across GPUs. Gradients are synced after the backward pass.\n",
    "\n",
    "✅ Advantages:\n",
    "- Easy to implement (e.g., torch.nn.DataParallel, DDP)\n",
    "- Scales well for small to mid-sized models\n",
    "- No model code changes required\n",
    "- Best for large data sets and small models\n",
    "\n",
    "❌ Disadvantages:\n",
    "- Inefficient for very large models (can't fit on one GPU)\n",
    "- All-reduce on gradients becomes a bottleneck at high scale\n",
    "\n",
    "\n",
    "Let's see it in action. We'll use a smaller model (1B) because we aren't splitting the model this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-14 17:01:34,462] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-14 17:01:34,477] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-14 17:01:34,489] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-14 17:01:34,497] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-14 17:01:37,424] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-14 17:01:37,424] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-14 17:01:37,424] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-14 17:01:37,593] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-14 17:01:37,593] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-14 17:01:37,593] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-14 17:01:37,707] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-14 17:01:37,707] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-14 17:01:37,707] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-14 17:01:37,807] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-14 17:01:37,807] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-14 17:01:37,807] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-14 17:01:38,047] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "ninja: no work to do.\n",
      "Time to load fused_adam op: 0.04162883758544922 seconds\n",
      "Time to load fused_adam op: 0.10139060020446777 seconds\n",
      "Time to load fused_adam op: 0.10097551345825195 seconds\n",
      "Time to load fused_adam op: 0.10143518447875977 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/fused_adam/build.ninja...\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-14 17:01:38,700] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-05-14 17:01:38,700] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-14 17:01:38,702] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-05-14 17:01:38,702] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2025-05-14 17:01:38,702] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 1 optimizer\n",
      "[2025-05-14 17:01:38,702] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000\n",
      "[2025-05-14 17:01:38,702] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000\n",
      "[2025-05-14 17:01:38,702] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2025-05-14 17:01:38,702] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2025-05-14 17:01:42,400] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-14 17:01:42,400] [INFO] [utils.py:782:see_memory_usage] MA 3.45 GB         Max_MA 4.03 GB         CA 4.03 GB         Max_CA 4 GB \n",
      "[2025-05-14 17:01:42,417] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.14 GB, percent = 4.5%\n",
      "[2025-05-14 17:01:42,531] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-14 17:01:42,532] [INFO] [utils.py:782:see_memory_usage] MA 3.45 GB         Max_MA 4.6 GB         CA 5.19 GB         Max_CA 5 GB \n",
      "[2025-05-14 17:01:42,532] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.99 GB, percent = 4.4%\n",
      "[2025-05-14 17:01:42,532] [INFO] [stage_1_and_2.py:544:__init__] optimizer state initialized\n",
      "[Rank 1] input_ids.shape: torch.Size([1, 8])\n",
      "[Rank 1] Allocated: 3.71 GB\n",
      "[Rank 1] Reserved:  5.57 GB\n",
      "[2025-05-14 17:01:42,640] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-14 17:01:42,640] [INFO] [utils.py:782:see_memory_usage] MA 3.45 GB         Max_MA 3.45 GB         CA 5.19 GB         Max_CA 5 GB \n",
      "[2025-05-14 17:01:42,640] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.01 GB, percent = 4.4%\n",
      "[2025-05-14 17:01:42,641] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-05-14 17:01:42,641] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-14 17:01:42,641] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-14 17:01:42,641] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f6d47208530>\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[Rank 2] input_ids.shape: torch.Size([1, 7])\n",
      "[2025-05-14 17:01:42,642] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[Rank 2] Allocated: 3.71 GB\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[Rank 2] Reserved:  5.57 GB[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   optimizer_name ............... adamw\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   steps_per_print .............. None\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   train_batch_size ............. 4\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   world_size ................... 4\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-14 17:01:42,643] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 1\n",
      "[2025-05-14 17:01:42,644] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.01\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 1\n",
      "    }, \n",
      "    \"tensor_parallel\": {\n",
      "        \"enabled\": true, \n",
      "        \"tp_size\": 4\n",
      "    }, \n",
      "    \"replace_with_kernel_inject\": false, \n",
      "    \"enable_cuda_graph\": false\n",
      "}\n",
      "\n",
      "🔁 Running batch size = 1\n",
      "[Rank 0] input_ids.shape: torch.Size([1, 8])\n",
      "[Rank 0] Allocated: 3.71 GB\n",
      "[Rank 0] Reserved:  5.57 GB\n",
      "[Rank 3] input_ids.shape: torch.Size([1, 7])\n",
      "[Rank 3] Allocated: 3.71 GB\n",
      "[Rank 3] Reserved:  5.57 GB\n",
      "[2025-05-14 17:01:44,331] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
      "[2025-05-14 17:01:45,212] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2025-05-14 17:01:46,104] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "Batch=1 | Seq=8+1\n",
      "Elapsed GPU time: 0.8916s | TFLOP/s: 0.0 | AI: 9.00 FLOP/B\n",
      "--------- OUTPUT BREAKDOWN ---------\n",
      "🧠 Tokens generated: 30\n",
      "⚡ Throughput: 33.64570 tokens/sec\n",
      "⏱️ Total time: 0.89164 sec\n",
      "💸 Cost per 1M tokens: $9.98972\n",
      "------------------------------------\n",
      "✅ Distributed env torn down and memory cleared.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "# Create a temporary deepspeed config file\n",
    "ds_config = {\n",
    "    \"train_micro_batch_size_per_gpu\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "\n",
    "    \"fp16\": { \"enabled\": True },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": 3e-5,\n",
    "            \"betas\": [0.9, 0.999],\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 0.01\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 1\n",
    "    },\n",
    "\n",
    "    \"tensor_parallel\": {\n",
    "        \"enabled\": True,\n",
    "        \"tp_size\": 4\n",
    "    },\n",
    "\n",
    "    \"replace_with_kernel_inject\": False,\n",
    "    \"enable_cuda_graph\": False\n",
    "}\n",
    "\n",
    "\n",
    "# # Running with 1 GPU\n",
    "# results = mutils.run_distributed_benchmark(\n",
    "#     model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "#     seq_len=32,\n",
    "#     min_new_tokens=1,\n",
    "#     batch_sizes=[100],\n",
    "#     dtype=torch.bfloat16,\n",
    "#     sharding=False,\n",
    "#     world_size=1, # Number of GPUs\n",
    "#     ds_config=ds_config\n",
    "# )\n",
    "# mutils.reset_distributed_and_clear_memory()\n",
    "\n",
    "# Copy the model to 2 GPUs and split the data\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=8,\n",
    "    min_new_tokens=1,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=4, # Number of GPUs\n",
    "    ds_config=ds_config\n",
    ")\n",
    "mutils.reset_distributed_and_clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As you can see distributed data can allow us to process our workload much faster, this is similar to how we previously used batching to improove price/performance. Here we can do something similar. But once our model is large enough this stragegy will no longer work, and we'll run into the same issues as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Parallism\n",
    "**How it works:**\n",
    "Individual layers are sharded across GPUs — e.g., split matrix rows/columns in linear layers.\n",
    "\n",
    "✅ Advantages:\n",
    "- Enables sharding of very large models/layers\n",
    "- Reduces per-GPU memory usage\n",
    "- Exploits fine-grained parallelism within layers\n",
    "\n",
    "❌ Disadvantages:\n",
    "- Requires deep model rewrites or tools like DeepSpeed/FSDP\n",
    "- Requires custom communication (e.g., all_gather, reduce_scatter)\n",
    "- Collective comms (e.g., NCCL) can dominate runtime if not optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 15:21:48,942] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-07 15:21:48,950] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "🔁 Running batch size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.48it/s]\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 15:21:51,329] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-07 15:21:51,329] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2025-05-07 15:21:51,329] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2025-05-07 15:21:51,332] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-07 15:21:51,371] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 5120, 'intermediate_size': 13824, 'heads': 40, 'num_hidden_layers': -1, 'dtype': torch.bfloat16, 'pre_layer_norm': True, 'norm_type': <NormType.RMSNorm: 3>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 2, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 128, 'rotate_half': True, 'rotate_every_two': False, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GATED_SILU: 4>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 33, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000.0, 'invert_mask': True}\n",
      "[2025-05-07 15:21:51,447] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-07 15:21:51,447] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2025-05-07 15:21:51,447] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2025-05-07 15:21:51,450] [INFO] [comm.py:652:init_distributed] cdb=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.31it/s]\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/transformer_inference/build.ninja...\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module transformer_inference...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/transformer_inference/build.ninja...\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module transformer_inference...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 0.020978927612304688 seconds\n",
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 0.02348494529724121 seconds\n",
      "------------------------------------------------------\n",
      "Free memory : 7.072205 (GigaBytes)  \n",
      "Total memory: 22.045044 (GigaBytes)  \n",
      "Requested memory: 0.015776 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 33 \n",
      "WorkSpace: 0x7f10c1400000 \n",
      "------------------------------------------------------\n",
      "Batch=1 | Seq=32+1\n",
      "Elapsed GPU time: 0.0664s | TFLOP/s: 6.6 | AI: 33.00 FLOP/B\n",
      "🧠 Tokens generated: 2\n",
      "⚡ Throughput: 30.05825 tokens/sec\n",
      "⏱️ Total time: 0.06654 sec\n",
      "💸 Cost per 1M tokens: $11.18199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"batch_size\": 1,\n",
      "    \"world_size\": 2,\n",
      "    \"avg_time_seconds\": 0.06635622406005859,\n",
      "    \"local_gflops\": 6636.48785683498,\n",
      "    \"aggregated_gflops\": 6636.48785683498,\n",
      "    \"total_flops\": 440372275200,\n",
      "    \"estimated_memory_bytes\": 13345280000,\n",
      "    \"arithmetic_intensity\": 32.998354114713216,\n",
      "    \"cost_per_1m_tokens\": 11.181992049225501\n",
      "  }\n",
      "]✅ Distributed env torn down and memory cleared.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "seq_len = 32\n",
    "min_new_tokens = 1\n",
    "world_size = 2\n",
    "max_tokens =  seq_len + min_new_tokens\n",
    "\n",
    "ds_config = {\n",
    "    \"replace_with_kernel_inject\": True,\n",
    "    \"enable_cuda_graph\": False,\n",
    "    \"tensor_parallel\": {\n",
    "        \"enabled\": True,\n",
    "        \"tp_size\": world_size\n",
    "    },\n",
    "    # Optional tuning knobs to constrain token planning\n",
    "    \"max_tokens\": max_tokens\n",
    "}\n",
    "\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Nous-Hermes-Llama2-13b\",\n",
    "    seq_len=32,\n",
    "    min_new_tokens=1,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=2, # Number of GPUs\n",
    "    ds_config=ds_config\n",
    ")\n",
    "mutils.reset_distributed_and_clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Tensor parallism is exactly the same as what we demonstrated in the last lab. This allows us to launch a much larger model by utilizing more GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Parallelism\n",
    "**How it works:**\n",
    "Each GPU holds a different stage of the model. Batches are split into micro-batches and passed between GPUs sequentially.\n",
    "\n",
    "✅ Advantages:\n",
    "Works well for extremely deep models\n",
    "- Spreads compute and memory across GPUs\n",
    "- Compatible with tensor parallelism for hybrid scaling\n",
    "\n",
    "❌ Disadvantages:\n",
    "- Latency due to pipeline bubbles (idle GPUs while others compute)\n",
    "- Complex micro-batching & scheduling\n",
    "- Harder to load balance if layers are uneven in cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 15:39:39,445] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-07 15:39:39,471] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "🔁 Running batch size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.82it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  7.67it/s]\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 15:39:42,517] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-07 15:39:42,573] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "[rank0]:[W507 15:39:42.046035231 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 403, in _distributed_worker\n    res = benchmark_batch_sizes(\n          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 57, in benchmark_batch_sizes\n    elapsed_s, tokens_generated, metrics, cost = benchmark_llm(\n                                                 ^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 201, in benchmark_llm\n    model = load_sharded_model(model_name, dtype, sharding, world_size, seq_len, min_new_tokens, ds_config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 144, in load_sharded_model\n    model = deepspeed.init_inference(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/deepspeed/__init__.py\", line 362, in init_inference\n    ds_inference_config = DeepSpeedInferenceConfig(**config_dict)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/deepspeed/runtime/config_utils.py\", line 57, in __init__\n    super().__init__(**data)\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/pydantic/main.py\", line 253, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 2 validation errors for DeepSpeedInferenceConfig\npipeline_parallel\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'enabled': True, 'pp_size': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npp_engine\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProcessRaisedException\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     10\u001b[39m max_tokens =  seq_len + min_new_tokens\n\u001b[32m     12\u001b[39m ds_config = {\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpipeline_parallel\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens\n\u001b[32m     18\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m results = \u001b[43mmutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_distributed_benchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNousResearch/Nous-Hermes-Llama2-13b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Number of GPUs\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mds_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_config\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m mutils.reset_distributed_and_clear_memory()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/environment/src/utils/model_utils.py:436\u001b[39m, in \u001b[36mrun_distributed_benchmark\u001b[39m\u001b[34m(model_name, seq_len, min_new_tokens, batch_sizes, dtype, sharding, world_size, ds_config)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Manager() \u001b[38;5;28;01mas\u001b[39;00m manager:\n\u001b[32m    434\u001b[39m     shared_results = manager.list()\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[43mmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_distributed_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m            \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m            \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m            \u001b[49m\u001b[43mds_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshared_results\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(shared_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:340\u001b[39m, in \u001b[36mspawn\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method)\u001b[39m\n\u001b[32m    334\u001b[39m     msg = (\n\u001b[32m    335\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    336\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    337\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    338\u001b[39m     )\n\u001b[32m    339\u001b[39m     warnings.warn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspawn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:296\u001b[39m, in \u001b[36mstart_processes\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method)\u001b[39m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:215\u001b[39m, in \u001b[36mProcessContext.join\u001b[39m\u001b[34m(self, timeout, grace_period)\u001b[39m\n\u001b[32m    213\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m % error_index\n\u001b[32m    214\u001b[39m msg += original_trace\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "\u001b[31mProcessRaisedException\u001b[39m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 403, in _distributed_worker\n    res = benchmark_batch_sizes(\n          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 57, in benchmark_batch_sizes\n    elapsed_s, tokens_generated, metrics, cost = benchmark_llm(\n                                                 ^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 201, in benchmark_llm\n    model = load_sharded_model(model_name, dtype, sharding, world_size, seq_len, min_new_tokens, ds_config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 144, in load_sharded_model\n    model = deepspeed.init_inference(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/deepspeed/__init__.py\", line 362, in init_inference\n    ds_inference_config = DeepSpeedInferenceConfig(**config_dict)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/deepspeed/runtime/config_utils.py\", line 57, in __init__\n    super().__init__(**data)\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/pydantic/main.py\", line 253, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 2 validation errors for DeepSpeedInferenceConfig\npipeline_parallel\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'enabled': True, 'pp_size': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npp_engine\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "seq_len = 32\n",
    "min_new_tokens = 1\n",
    "world_size = 2\n",
    "max_tokens =  seq_len + min_new_tokens\n",
    "\n",
    "ds_config = {\n",
    "    \"pipeline_parallel\": {\n",
    "        \"enabled\": True,\n",
    "        \"pp_size\": world_size\n",
    "    },\n",
    "    \"max_tokens\": max_tokens\n",
    "}\n",
    "\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Nous-Hermes-Llama2-13b\",\n",
    "    seq_len=32,\n",
    "    min_new_tokens=1,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=\"tensor\",\n",
    "    world_size=2, # Number of GPUs\n",
    "    ds_config=ds_config\n",
    ")\n",
    "mutils.reset_distributed_and_clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
