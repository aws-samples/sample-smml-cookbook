{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2\n",
    "## Lab 2: Back to Libraries\n",
    "\n",
    "Now that we understand HOW sharding is done, we can understand how this tool is utilized by  libaries to optimize your model inference/training across multiple accelerators. If you want a deeper dive on individual sharding patterns that make up parallism patterns, and the collectives associated with them, we highly recommend [this chapter](https://jax-ml.github.io/scaling-book/sharding/) of **How to Scale your Model**.\n",
    "\n",
    "In this notebook though, we will focus on the different parallism strategies that can be implemented by these libaries, and what the considerations with them are.\n",
    "\n",
    "We'll use a 1B parameter model for demonstration purposes, but will show how you can combine these techniques with a larger model at the end.\n",
    "\n",
    "### Parallism Patterns\n",
    "This lab will be a hands on representation of common parallism patterns. If you want a achemic deep dive, we suggest you read through [The Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high_level_overview).\n",
    "\n",
    "> Note we will be using Deepspeed here but this could just as easily be done with any distributed inference/training framework, they have similar interfaces\n",
    "\n",
    "We will be covering the following:\n",
    "- Data Parallism\n",
    "- ZeRO-3/FSDP (these can be interchangable in most contexts)\n",
    "- Tensor Parallism (Like we covered last chapter)\n",
    "\n",
    "There are other strategies as well, and likely more will emerge, but these are the main ones and others will follow similar concepts. Throughout the first part of this module we'll still be using a small model to demonstrate different types of parallism. At the end we will combine these strategies to run a much larger model on the same instance!\n",
    "\n",
    "The concepts we cover in this module can scale to thousands of GPUs.\n",
    "\n",
    "Let's start by installing our libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ec2-user/.local/lib/python3.12/site-packages (4.47.1)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/.local/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: deepspeed in /home/ec2-user/.local/lib/python3.12/site-packages (0.16.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/.local/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/.local/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: einops in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (0.8.1)\n",
      "Requirement already satisfied: hjson in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (3.1.0)\n",
      "Requirement already satisfied: msgpack in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (1.1.0)\n",
      "Requirement already satisfied: ninja in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (1.11.1.4)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (2.11.4)\n",
      "Requirement already satisfied: nvidia-ml-py in /home/ec2-user/.local/lib/python3.12/site-packages (from deepspeed) (12.570.86)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from pydantic>=2.0.0->deepspeed) (0.4.0)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (80.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/.local/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/.local/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers accelerate deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run through the different types of parallism. Keep an eye out for `OUTPUT BREAKDOWN` to see what the token generation and cost looks like. You may see some warnings/errors but these are benign and can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "Before we get started we'll provide an appendix that describes the different images we'll be going through to visualize the types of parallism, their advantages and disadvantages.\n",
    "\n",
    "![](./assets/appendix.png)\n",
    "\n",
    "- The A Matrix represents data being input, as well as it's sequence length and batch size depending on how large it is\n",
    "- The B matrix represents the parameters for the model\n",
    "- The training states represent addition memory needed for forward/backward pass in training, in inference this isn't relevant, and you essentially only have the forward pass\n",
    "- We will be using GPUs but this could work with Neuron devices or any other accelerator\n",
    "- We won't be using 2x2 topologies, but in reality, that's how you'd model out your GPU topology when using multiple types of parallism\n",
    "\n",
    "With that out of the way let's jump into data parallism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Parallel\n",
    "![](./assets/dp.png)\n",
    "**How it works:**\n",
    "Each GPU has a full copy of the model. Batches are split across GPUs. Gradients are synced after the backward pass. As you can see this affectively just splits the input data. If your model is too large, or the training states take up too much memory this won't help reduce your memory footprint. It's best used as a tool to improve throughput.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "- Easy to implement (e.g., torch.nn.DataParallel, DDP)\n",
    "- Scales well for small to mid-sized models\n",
    "- No model code changes required\n",
    "- Best for large data sets and small models\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "- Inefficient for very large models (can't fit on one GPU)\n",
    "- All-reduce on gradients becomes a bottleneck at high scale\n",
    "\n",
    "\n",
    "Let's see it in action. We'll use a smaller model (1B) because we aren't splitting the model this time.\n",
    "\n",
    "> Note: Deepspeed does this automatically as you provide basic optimization and multiple GPUs, as it's a standard optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 15:13:30,113] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:13:33,935] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "üîÅ Running batch size = 4\n",
      "Batch=4 | SeqLen=8\n",
      "Elapsed GPU time: 0.5126s | TFLOP/s: 1.5 | AI: 22.86 FLOP/B\n",
      "--------- OUTPUT BREAKDOWN ---------\n",
      "üß† Tokens generated: 32\n",
      "‚ö° Throughput: 62.42335 tokens/sec\n",
      "‚è±Ô∏è Total time: 0.51263 sec\n",
      "üí∏ Cost per 1M tokens: $5.38438\n",
      "------------------------------------\n",
      "‚úÖ Distributed env torn down and memory cleared.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/multiprocessing/resource_tracker.py:123: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 15:13:46,100] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:13:46,149] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:13:46,174] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:13:46,199] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:13:48,552] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-27 15:13:48,552] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-27 15:13:48,735] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-27 15:13:48,749] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-27 15:13:48,769] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-27 15:13:48,831] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-27 15:13:48,842] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-27 15:13:48,869] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-27 15:13:48,870] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized\n",
      "[2025-05-27 15:13:48,870] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-27 15:13:48,895] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-27 15:13:49,648] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.284883499145508 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-27 15:13:53,075] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "Time to load cpu_adam op: 2.377256393432617 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "Time to load cpu_adam op: 2.3785486221313477 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "Time to load cpu_adam op: 2.378740072250366 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-27 15:13:53,165] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-05-27 15:13:53,165] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-27 15:13:53,167] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2025-05-27 15:13:53,167] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2025-05-27 15:13:53,167] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2025-05-27 15:13:53,167] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float32 ZeRO stage 3 optimizer\n",
      "[2025-05-27 15:13:53,168] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-27 15:13:53,168] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 15:13:53,288] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "[2025-05-27 15:13:53,289] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:13:53,289] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.63 GB, percent = 4.2%\n",
      "[2025-05-27 15:13:53,290] [INFO] [stage3.py:168:__init__] Reduce bucket size 500000000\n",
      "[2025-05-27 15:13:53,290] [INFO] [stage3.py:169:__init__] Prefetch bucket size 50000000\n",
      "[2025-05-27 15:13:53,396] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-05-27 15:13:53,396] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:13:53,396] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 7.63 GB, percent = 4.2%\n",
      "[2025-05-27 15:13:53,398] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "Parameter Offload: Total persistent parameters: 67584 in 33 params\n",
      "[2025-05-27 15:13:55,726] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2025-05-27 15:13:55,727] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:13:55,727] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 10.34 GB, percent = 5.7%\n",
      "[2025-05-27 15:13:55,836] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
      "[2025-05-27 15:13:55,837] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:13:55,837] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 10.34 GB, percent = 5.7%\n",
      "[2025-05-27 15:13:57,860] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2025-05-27 15:13:57,861] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:13:57,861] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.48 GB, percent = 9.6%\n",
      "[2025-05-27 15:13:58,008] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
      "[2025-05-27 15:13:58,009] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:13:58,009] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 17.88 GB, percent = 9.8%\n",
      "[2025-05-27 15:13:58,239] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
      "[2025-05-27 15:13:58,240] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:13:58,240] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 19.04 GB, percent = 10.5%\n",
      "[2025-05-27 15:13:58,465] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-27 15:13:58,466] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:13:58,466] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 25.15 GB, percent = 13.8%\n",
      "[2025-05-27 15:14:00,134] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-27 15:14:00,135] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:14:00,135] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.2 GB, percent = 15.0%\n",
      "[2025-05-27 15:14:00,135] [INFO] [stage3.py:528:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2025-05-27 15:14:02,414] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-27 15:14:02,415] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 1.91 GB         CA 3.73 GB         Max_CA 4 GB \n",
      "[2025-05-27 15:14:02,415] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 36.67 GB, percent = 20.2%\n",
      "[2025-05-27 15:14:02,415] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\n",
      "[2025-05-27 15:14:02,415] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-27 15:14:02,415] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-27 15:14:02,415] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fea2a9527b0>\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-05-27 15:14:02,416] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   fp16_enabled ................. False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   optimizer_name ............... adamw\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-05-27 15:14:02,417] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   steps_per_print .............. None\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   train_batch_size ............. 4\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   world_size ................... 4\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\n",
      "[2025-05-27 15:14:02,418] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.01\n",
      "        }\n",
      "    }, \n",
      "    \"replace_with_kernel_inject\": false, \n",
      "    \"enable_cuda_graph\": false, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }\n",
      "}\n",
      "\n",
      "üîÅ Running batch size = 16\n",
      "Batch=16 | SeqLen=32\n",
      "Elapsed GPU time: 13.3362s | TFLOP/s: 0.2 | AI: 363.85 FLOP/B\n",
      "Batch=16 | SeqLen=32\n",
      "Elapsed GPU time: 13.3699s | TFLOP/s: 0.2 | AI: 363.85 FLOP/B\n",
      "Batch=16 | SeqLen=32\n",
      "Elapsed GPU time: 13.3710s | TFLOP/s: 0.2 | AI: 363.85 FLOP/B\n",
      "Batch=16 | SeqLen=32\n",
      "Elapsed GPU time: 13.3852s | TFLOP/s: 0.2 | AI: 363.85 FLOP/B\n",
      "--------- OUTPUT BREAKDOWN ---------\n",
      "üß† Tokens generated: 2048\n",
      "‚ö° Throughput: 153.22926 tokens/sec\n",
      "‚è±Ô∏è Total time: 13.36559 sec\n",
      "üí∏ Cost per 1M tokens: $2.19352\n",
      "------------------------------------\n",
      "‚úÖ Distributed env torn down and memory cleared.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "# Generate a DeepSpeed config with key overrides only\n",
    "# Using batch size 2, grad accumulation 2, and no fp16\n",
    "ds_config = mutils.make_ds_config(\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "# Run single-GPU benchmark (no sharding)\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=8,\n",
    "    batch_sizes=[4],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=False,\n",
    "    world_size=1,\n",
    "    ds_config=ds_config\n",
    ")\n",
    "\n",
    "mutils.reset_distributed_and_clear_memory()\n",
    "\n",
    "# Run multi-GPU benchmark with sharding across 4 GPUs with a bigger batch size\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=32,\n",
    "    batch_sizes=[16],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=4,\n",
    "    ds_config=ds_config\n",
    ")\n",
    "\n",
    "mutils.reset_distributed_and_clear_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As you can see distributed data can allow us to process our workload much faster, this is similar to how we previously used batching to improove price/performance. Here we can do something similar. But once our model is large enough this stragegy will no longer work, and we'll run into the same issues as before. So we'll have to find ways to either reduce the model size or reduce the training states (in most cases both)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZeRO-3/FSDP\n",
    "![](./assets/zero.png)\n",
    "**How it works:**\n",
    "ZeRO-3 (and FSDP) shards all model states ‚Äî including parameters, gradients, and optimizer states ‚Äî across GPUs. It avoids redundancy by ensuring that no GPU holds a full copy of the model. Parameters are temporarily reconstructed using AllGather at compute time.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "- Shards the entire model, including non-linear layers, embeddings, and optimizer states\n",
    "- Enables training models that exceed per-GPU memory\n",
    "- Integrates with existing PyTorch models via FSDP or DeepSpeed ZeRO\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "- Requires full parameter AllGather before each forward/backward step\n",
    "- Communication-intensive (especially with many GPUs)\n",
    "- Can be slower without high-bandwidth interconnect (e.g., NVLink or InfiniBand)\n",
    "\n",
    "> We will often stack ZeRO-3/FSDP with other strategies like Tensor and Data Parallelism. For example, we may shard linear layers with TP while still using ZeRO-3 to handle the remaining memory overhead from unsharded layers and optimizer state. This allows us to scale across both memory and compute bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/multiprocessing/resource_tracker.py:123: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 15:16:14,285] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:16:14,295] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:16:16,635] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-27 15:16:16,636] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-27 15:16:16,686] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-27 15:16:16,790] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-05-27 15:16:16,851] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-27 15:16:16,851] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized\n",
      "[2025-05-27 15:16:16,851] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2025-05-27 15:16:17,799] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-27 15:16:18,935] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "[2025-05-27 15:16:18,935] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.287997007369995 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-27 15:16:21,228] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Time to load cpu_adam op: 2.37626314163208 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-27 15:16:21,312] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-05-27 15:16:21,312] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-27 15:16:21,314] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2025-05-27 15:16:21,314] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2025-05-27 15:16:21,314] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2025-05-27 15:16:21,314] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 15:16:21,436] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "[2025-05-27 15:16:21,436] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:16:21,437] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 6.84 GB, percent = 3.8%\n",
      "[2025-05-27 15:16:21,437] [INFO] [stage3.py:168:__init__] Reduce bucket size 500000000\n",
      "[2025-05-27 15:16:21,437] [INFO] [stage3.py:169:__init__] Prefetch bucket size 50000000\n",
      "[2025-05-27 15:16:21,545] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-05-27 15:16:21,545] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:16:21,545] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 6.84 GB, percent = 3.8%\n",
      "[2025-05-27 15:16:21,547] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Parameter Offload: Total persistent parameters: 67584 in 33 params\n",
      "[2025-05-27 15:16:22,522] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2025-05-27 15:16:22,522] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:16:22,522] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.34 GB, percent = 5.1%\n",
      "[2025-05-27 15:16:22,632] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
      "[2025-05-27 15:16:22,633] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:16:22,633] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.34 GB, percent = 5.1%\n",
      "[2025-05-27 15:16:23,946] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2025-05-27 15:16:23,948] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:16:23,949] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 16.11 GB, percent = 8.9%\n",
      "[2025-05-27 15:16:24,065] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
      "[2025-05-27 15:16:24,066] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:16:24,066] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 15.73 GB, percent = 8.7%\n",
      "[2025-05-27 15:16:24,400] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
      "[2025-05-27 15:16:24,400] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:16:24,400] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 18.02 GB, percent = 9.9%\n",
      "[2025-05-27 15:16:24,557] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-27 15:16:24,557] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:16:24,557] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.61 GB, percent = 12.4%\n",
      "[2025-05-27 15:16:27,319] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-27 15:16:27,319] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:16:27,319] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 26.19 GB, percent = 14.4%\n",
      "[2025-05-27 15:16:27,320] [INFO] [stage3.py:528:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2025-05-27 15:16:29,486] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-27 15:16:29,486] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 1.91 GB         CA 3.73 GB         Max_CA 4 GB \n",
      "[2025-05-27 15:16:29,487] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 31.3 GB, percent = 17.2%\n",
      "[2025-05-27 15:16:29,487] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\n",
      "[2025-05-27 15:16:29,487] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-27 15:16:29,487] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-27 15:16:29,487] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2025-05-27 15:16:29,487] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff0f9c09c10>\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-05-27 15:16:29,488] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   optimizer_name ............... adamw\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   steps_per_print .............. None\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   train_batch_size ............. 2\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\n",
      "[2025-05-27 15:16:29,489] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.01\n",
      "        }\n",
      "    }, \n",
      "    \"replace_with_kernel_inject\": false, \n",
      "    \"enable_cuda_graph\": false, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }\n",
      "}\n",
      "\n",
      "üîÅ Running batch size = 1\n",
      "[2025-05-27 15:16:32,813] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
      "[2025-05-27 15:16:32,814] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "[2025-05-27 15:16:35,258] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2025-05-27 15:16:35,259] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "Batch=1 | SeqLen=32\n",
      "Elapsed GPU time: 1.6486s | TFLOP/s: 0.1 | AI: 22.85 FLOP/B\n",
      "Batch=1 | SeqLen=32\n",
      "Elapsed GPU time: 1.6491s | TFLOP/s: 0.1 | AI: 22.85 FLOP/B\n",
      "--------- OUTPUT BREAKDOWN ---------\n",
      "üß† Tokens generated: 64\n",
      "‚ö° Throughput: 38.81548 tokens/sec\n",
      "‚è±Ô∏è Total time: 1.64883 sec\n",
      "üí∏ Cost per 1M tokens: $8.65920\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "seq_len = 32\n",
    "min_new_tokens = 1\n",
    "world_size = 2\n",
    "\n",
    "# We use zero and offload additional memory to CPU\n",
    "ds_config = mutils.make_ds_config(\n",
    "    zero_config={\n",
    "        \"stage\": 3,\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"overlap_comm\": True\n",
    "    })\n",
    "\n",
    "# Run benchmark with the generated config\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=seq_len,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=world_size,\n",
    "    ds_config=ds_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Parallism\n",
    "![](./assets/tp.png)\n",
    "**How it works:**\n",
    "Individual layers are sharded across GPUs ‚Äî e.g., split matrix rows/columns in linear layers. Typically this requires a custom implementation of the model for parallism. This is usually done for popular models by frameworks like Pytorch and Deepspeed, but keep this in mind when using cutting edge models or creating your own, if the architecure is unique the model definition will need to account for this.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "- Enables sharding of very large models/layers\n",
    "- Reduces per-GPU memory usage\n",
    "- Exploits fine-grained parallelism within layers\n",
    "- Less CC than CC heavy DP\n",
    "- Reduces compute\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "- Requires deep model rewrites or tools like DeepSpeed/FSDP\n",
    "- Requires custom communication (e.g., all_gather, reduce_scatter)\n",
    "- CC (e.g., NCCL) can dominate runtime if not optimized or scaled too high\n",
    "\n",
    "> We will stack types of parallism on top of each other as by themselves they may not be enough to to store in memory. For example, we will stack DP and TP in this case. You will see DP, and TP moving forward as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 15:16:56,793] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:16:56,808] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:16:56,856] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:16:56,868] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 15:16:59,285] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-27 15:16:59,285] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-27 15:16:59,390] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-27 15:16:59,413] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-27 15:16:59,421] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-27 15:16:59,485] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-27 15:16:59,524] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-27 15:16:59,545] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-27 15:16:59,571] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-27 15:16:59,571] [INFO] [comm.py:677:init_distributed] Distributed backend already initialized\n",
      "[2025-05-27 15:16:59,571] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-27 15:17:01,423] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-27 15:17:02,557] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "[2025-05-27 15:17:02,558] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "[2025-05-27 15:17:02,558] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "[2025-05-27 15:17:02,560] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 12.6 does not match the version torch was compiled with 12.4 but since the APIs are compatible, accepting this combination\n",
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.2884745597839355 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-27 15:17:04,852] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "Time to load cpu_adam op: 2.3761134147644043 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-27 15:17:04,935] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
      "[2025-05-27 15:17:04,935] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-27 15:17:04,937] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2025-05-27 15:17:04,938] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2025-05-27 15:17:04,938] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2025-05-27 15:17:04,938] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "Time to load cpu_adam op: 2.379671573638916 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "Time to load cpu_adam op: 2.379852771759033 seconds\n",
      "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
      "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2025-05-27 15:17:04,942] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "[2025-05-27 15:17:04,945] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 15:17:05,062] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\n",
      "[2025-05-27 15:17:05,063] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:17:05,063] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.89 GB, percent = 4.9%\n",
      "[2025-05-27 15:17:05,064] [INFO] [stage3.py:168:__init__] Reduce bucket size 500000000\n",
      "[2025-05-27 15:17:05,064] [INFO] [stage3.py:169:__init__] Prefetch bucket size 50000000\n",
      "[2025-05-27 15:17:05,175] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2025-05-27 15:17:05,175] [INFO] [utils.py:782:see_memory_usage] MA 2.3 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:17:05,175] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 8.88 GB, percent = 4.9%\n",
      "[2025-05-27 15:17:05,177] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 4\n",
      "Parameter Offload: Total persistent parameters: 67584 in 33 params\n",
      "[2025-05-27 15:17:07,515] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2025-05-27 15:17:07,515] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 2.3 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:17:07,516] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 11.59 GB, percent = 6.4%\n",
      "[2025-05-27 15:17:07,626] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\n",
      "[2025-05-27 15:17:07,627] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:17:07,627] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 11.59 GB, percent = 6.4%\n",
      "[2025-05-27 15:17:09,827] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2025-05-27 15:17:09,827] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:17:09,828] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 19.13 GB, percent = 10.5%\n",
      "[2025-05-27 15:17:09,938] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\n",
      "[2025-05-27 15:17:09,939] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:17:09,939] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 19.13 GB, percent = 10.5%\n",
      "[2025-05-27 15:17:10,170] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\n",
      "[2025-05-27 15:17:10,170] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:17:10,170] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 20.28 GB, percent = 11.2%\n",
      "[2025-05-27 15:17:10,471] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-27 15:17:10,472] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:17:10,472] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 27.51 GB, percent = 15.1%\n",
      "[2025-05-27 15:17:12,043] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-27 15:17:12,043] [INFO] [utils.py:782:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 2.3 GB         Max_CA 2 GB \n",
      "[2025-05-27 15:17:12,044] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 28.45 GB, percent = 15.7%\n",
      "[2025-05-27 15:17:12,044] [INFO] [stage3.py:528:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2025-05-27 15:17:13,578] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-27 15:17:13,579] [INFO] [utils.py:782:see_memory_usage] MA 0.93 GB         Max_MA 1.91 GB         CA 3.73 GB         Max_CA 4 GB \n",
      "[2025-05-27 15:17:13,579] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 33.72 GB, percent = 18.6%\n",
      "[2025-05-27 15:17:13,579] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\n",
      "[2025-05-27 15:17:13,579] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-27 15:17:13,579] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-27 15:17:13,579] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-27 15:17:13,580] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f619ef14e00>\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   optimizer_name ............... adamw\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.01}\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2025-05-27 15:17:13,581] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   steps_per_print .............. None\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   train_batch_size ............. 4\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   world_size ................... 4\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\n",
      "[2025-05-27 15:17:13,582] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 3e-05, \n",
      "            \"betas\": [0.9, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.01\n",
      "        }\n",
      "    }, \n",
      "    \"replace_with_kernel_inject\": false, \n",
      "    \"enable_cuda_graph\": false, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"tensor_parallel\": {\n",
      "        \"enabled\": true, \n",
      "        \"tp_size\": 4\n",
      "    }\n",
      "}\n",
      "\n",
      "üîÅ Running batch size = 1\n",
      "[2025-05-27 15:17:18,124] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
      "[2025-05-27 15:17:18,124] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "[2025-05-27 15:17:21,677] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2025-05-27 15:17:21,677] [INFO] [stage3.py:2024:_loco_err_buf_update] update loco-zero++ error buffer with overflow: True\n",
      "Batch=1 | SeqLen=32\n",
      "Elapsed GPU time: 2.7553s | TFLOP/s: 0.1 | AI: 22.85 FLOP/B\n",
      "Batch=1 | SeqLen=32\n",
      "Elapsed GPU time: 2.8083s | TFLOP/s: 0.1 | AI: 22.85 FLOP/B\n",
      "Batch=1 | SeqLen=32\n",
      "Elapsed GPU time: 2.8241s | TFLOP/s: 0.1 | AI: 22.85 FLOP/B\n",
      "Batch=1 | SeqLen=32\n",
      "Elapsed GPU time: 2.8364s | TFLOP/s: 0.1 | AI: 22.85 FLOP/B\n",
      "--------- OUTPUT BREAKDOWN ---------\n",
      "üß† Tokens generated: 128\n",
      "‚ö° Throughput: 45.61640 tokens/sec\n",
      "‚è±Ô∏è Total time: 2.80601 sec\n",
      "üí∏ Cost per 1M tokens: $7.36821\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "seq_len = 32\n",
    "min_new_tokens = 1\n",
    "world_size = 4\n",
    "\n",
    "# Generate DeepSpeed config with FP16 and tensor parallelism across 4 GPUs\n",
    "ds_config = mutils.make_ds_config(\n",
    "    tensor_parallel=world_size\n",
    ")\n",
    "\n",
    "# Run benchmark using tensor parallelism\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=seq_len,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=world_size,\n",
    "    ds_config=ds_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Tensor parallism is exactly the same as what we demonstrated in the last lab. This allows us to launch a much larger model by utilizing more GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cases\n",
    "We've discussed some of the different approaches to parallism for models at scale, but they don't work in a vacuum. As you already saw, you can stack some of these strategies to get the best of both worlds. \n",
    "\n",
    "## Large Model Use Case\n",
    "The following is an example of how you could scale a large model by leveraging ZeRO-3, DP, and TP together. \n",
    "\n",
    "![](./assets/large_model.png)\n",
    "\n",
    "Stacking allows us to get memory savings, data savings, and optimization state saving across GPUs without overutilizing collective compute from ZeRO.\n",
    "\n",
    "## Big Data Use Case\n",
    "What about use cases where the model is small? In these cases we can focus only on DP, as ZeRO-3 and TP would add unnecessary communication, and the bottleneck is the data.\n",
    "\n",
    "![](./assets/big_data.png)\n",
    "\n",
    "As you can see we get most of the value from DP, and there's no need to introduce other forms of parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The concepts introduced here don't cover all forms of parallelism, but foundationally when chosing how to scale a model across accelerators, consider what the tradeoffs of different strategies are, and the engineering effort required to implement them. Other forms of parallelism like Pipeline Parallelism can be further intrdocued to scale your workloads, but require even more invasive engineering.\n",
    "\n",
    "When developing a platform or speaking to those who are, understand the use case for the model, and the priorities when it comes to scaling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
