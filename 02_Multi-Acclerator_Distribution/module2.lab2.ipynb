{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2\n",
    "## Lab 2: Back to Libraries\n",
    "\n",
    "Now that we understand HOW sharding is done, we can understand how this tool is utilized by  libaries to optimize your model inference/training across multiple accelerators. If you want a deeper dive on individual sharding patterns that make up parallism patterns, and the collectives associated with them, we highly recommend [this chapter](https://jax-ml.github.io/scaling-book/sharding/) of **How to Scale your Model**.\n",
    "\n",
    "In this notebook though, we will focus on the different parallism strategies that can be implemented by these libaries, and what the considerations with them are.\n",
    "\n",
    "### Parallism Patterns\n",
    "This lab will be a hands on representation of common parallism patterns. If you want a achemic deep dive, we suggest you read through [The Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=high_level_overview).\n",
    "\n",
    "> Note we will be using Deepspeed here but this could just as easily be done with any distributed inference/training framework, they have similar interfaces\n",
    "\n",
    "We will be covering the following:\n",
    "- Data Parallism\n",
    "- Tensor Parallism (Like we covered last chapter)\n",
    "- Pipeline Parallism\n",
    "- ZeRO\n",
    "\n",
    "There are other strategies as well, and likely more will emerge, but these are the main ones and others will follow similar concepts. \n",
    "\n",
    "Let's start by installing our libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run through the different types of parallism. Keep an eye out for `OUTPUT BREAKDOWN` to see what the token generation and cost looks like. You may see some warnings/errors but these are benign and can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Parallel\n",
    "**How it works:**\n",
    "Each GPU has a full copy of the model. Batches are split across GPUs. Gradients are synced after the backward pass.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "- Easy to implement (e.g., torch.nn.DataParallel, DDP)\n",
    "- Scales well for small to mid-sized models\n",
    "- No model code changes required\n",
    "- Best for large data sets and small models\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "- Inefficient for very large models (can't fit on one GPU)\n",
    "- All-reduce on gradients becomes a bottleneck at high scale\n",
    "\n",
    "\n",
    "Let's see it in action. We'll use a smaller model (1B) because we aren't splitting the model this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 15:51:40,921] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "üîÅ Running batch size = 100\n",
      "Batch=100 | Seq=32+1\n",
      "Elapsed GPU time: 0.1788s | TFLOP/s: 45.6 | AI: 3264.83 FLOP/B\n",
      "--------- OUTPUT BREAKDOWN ---------\n",
      "üß† Tokens generated: 100\n",
      "‚ö° Throughput: 559.22767 tokens/sec\n",
      "‚è±Ô∏è Total time: 0.17882 sec\n",
      "üí∏ Cost per 1M tokens: $0.60103\n",
      "------------------------------------\n",
      "‚úÖ Distributed env torn down and memory cleared.\n",
      "[2025-05-07 15:51:48,979] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-07 15:51:49,000] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "üîÅ Running batch size = 100\n",
      "[2025-05-07 15:51:51,221] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-07 15:51:51,221] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2025-05-07 15:51:51,221] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2025-05-07 15:51:51,222] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-07 15:51:51,223] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2025-05-07 15:51:51,223] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "Batch=50 | Seq=32+1\n",
      "Elapsed GPU time: 0.0913s | TFLOP/s: 44.7 | AI: 1641.16 FLOP/B\n",
      "--------- OUTPUT BREAKDOWN ---------\n",
      "üß† Tokens generated: 100\n",
      "‚ö° Throughput: 1086.07705 tokens/sec\n",
      "‚è±Ô∏è Total time: 0.09207 sec\n",
      "üí∏ Cost per 1M tokens: $0.30947\n",
      "------------------------------------\n",
      "‚úÖ Distributed env torn down and memory cleared.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "# Create a temporary deepspeed config file\n",
    "ds_config = {\n",
    "    \"tensor_parallel\": {\n",
    "        \"enabled\": False\n",
    "    },\n",
    "    \"enable_cuda_graph\": False,\n",
    "}\n",
    "\n",
    "# Running with 1 GPU\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=32,\n",
    "    min_new_tokens=1,\n",
    "    batch_sizes=[100],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=False,\n",
    "    world_size=1, # Number of GPUs\n",
    "    ds_config=ds_config\n",
    ")\n",
    "mutils.reset_distributed_and_clear_memory()\n",
    "\n",
    "# Copy the model to 2 GPUs and split the data\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Llama-3.2-1B\",\n",
    "    seq_len=32,\n",
    "    min_new_tokens=1,\n",
    "    batch_sizes=[100],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=2, # Number of GPUs\n",
    "    ds_config=ds_config\n",
    ")\n",
    "mutils.reset_distributed_and_clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! As you can see distributed data can allow us to process our workload much faster, this is similar to how we previously used batching to improove price/performance. Here we can do something similar. But once our model is large enough this stragegy will no longer work, and we'll run into the same issues as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Parallism\n",
    "**How it works:**\n",
    "Individual layers are sharded across GPUs ‚Äî e.g., split matrix rows/columns in linear layers.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "- Enables sharding of very large models/layers\n",
    "- Reduces per-GPU memory usage\n",
    "- Exploits fine-grained parallelism within layers\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "- Requires deep model rewrites or tools like DeepSpeed/FSDP\n",
    "- Requires custom communication (e.g., all_gather, reduce_scatter)\n",
    "- Collective comms (e.g., NCCL) can dominate runtime if not optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 15:21:48,942] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-07 15:21:48,950] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "üîÅ Running batch size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  7.48it/s]\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 15:21:51,329] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-07 15:21:51,329] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2025-05-07 15:21:51,329] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2025-05-07 15:21:51,332] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2025-05-07 15:21:51,371] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 5120, 'intermediate_size': 13824, 'heads': 40, 'num_hidden_layers': -1, 'dtype': torch.bfloat16, 'pre_layer_norm': True, 'norm_type': <NormType.RMSNorm: 3>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 2, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 128, 'rotate_half': True, 'rotate_every_two': False, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GATED_SILU: 4>, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 33, 'min_out_tokens': 1, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': False, 'transposed_mode': False, 'use_triton': False, 'triton_autotune': False, 'num_kv': -1, 'rope_theta': 10000.0, 'invert_mask': True}\n",
      "[2025-05-07 15:21:51,447] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-07 15:21:51,447] [WARNING] [config_utils.py:70:_process_deprecated_field] Config parameter replace_method is deprecated. This parameter is no longer needed, please remove from your call to DeepSpeed-inference\n",
      "[2025-05-07 15:21:51,447] [INFO] [logging.py:128:log_dist] [Rank -1] quantize_bits = 8 mlp_extra_grouping = False, quantize_groups = 1\n",
      "[2025-05-07 15:21:51,450] [INFO] [comm.py:652:init_distributed] cdb=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  7.31it/s]\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/transformer_inference/build.ninja...\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module transformer_inference...\n",
      "Using /home/ec2-user/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ec2-user/.cache/torch_extensions/py312_cu124/transformer_inference/build.ninja...\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module transformer_inference...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 0.020978927612304688 seconds\n",
      "ninja: no work to do.\n",
      "Time to load transformer_inference op: 0.02348494529724121 seconds\n",
      "------------------------------------------------------\n",
      "Free memory : 7.072205 (GigaBytes)  \n",
      "Total memory: 22.045044 (GigaBytes)  \n",
      "Requested memory: 0.015776 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 33 \n",
      "WorkSpace: 0x7f10c1400000 \n",
      "------------------------------------------------------\n",
      "Batch=1 | Seq=32+1\n",
      "Elapsed GPU time: 0.0664s | TFLOP/s: 6.6 | AI: 33.00 FLOP/B\n",
      "üß† Tokens generated: 2\n",
      "‚ö° Throughput: 30.05825 tokens/sec\n",
      "‚è±Ô∏è Total time: 0.06654 sec\n",
      "üí∏ Cost per 1M tokens: $11.18199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/multiprocessing/synchronize.py\", line 87, in _cleanup\n",
      "    sem_unlink(name)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"batch_size\": 1,\n",
      "    \"world_size\": 2,\n",
      "    \"avg_time_seconds\": 0.06635622406005859,\n",
      "    \"local_gflops\": 6636.48785683498,\n",
      "    \"aggregated_gflops\": 6636.48785683498,\n",
      "    \"total_flops\": 440372275200,\n",
      "    \"estimated_memory_bytes\": 13345280000,\n",
      "    \"arithmetic_intensity\": 32.998354114713216,\n",
      "    \"cost_per_1m_tokens\": 11.181992049225501\n",
      "  }\n",
      "]‚úÖ Distributed env torn down and memory cleared.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "seq_len = 32\n",
    "min_new_tokens = 1\n",
    "world_size = 2\n",
    "max_tokens =  seq_len + min_new_tokens\n",
    "\n",
    "ds_config = {\n",
    "    \"replace_with_kernel_inject\": True,\n",
    "    \"enable_cuda_graph\": False,\n",
    "    \"tensor_parallel\": {\n",
    "        \"enabled\": True,\n",
    "        \"tp_size\": world_size\n",
    "    },\n",
    "    # Optional tuning knobs to constrain token planning\n",
    "    \"max_tokens\": max_tokens\n",
    "}\n",
    "\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Nous-Hermes-Llama2-13b\",\n",
    "    seq_len=32,\n",
    "    min_new_tokens=1,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=True,\n",
    "    world_size=2, # Number of GPUs\n",
    "    ds_config=ds_config\n",
    ")\n",
    "mutils.reset_distributed_and_clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Tensor parallism is exactly the same as what we demonstrated in the last lab. This allows us to launch a much larger model by utilizing more GPUs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Parallelism\n",
    "**How it works:**\n",
    "Each GPU holds a different stage of the model. Batches are split into micro-batches and passed between GPUs sequentially.\n",
    "\n",
    "‚úÖ Advantages:\n",
    "Works well for extremely deep models\n",
    "- Spreads compute and memory across GPUs\n",
    "- Compatible with tensor parallelism for hybrid scaling\n",
    "\n",
    "‚ùå Disadvantages:\n",
    "- Latency due to pipeline bubbles (idle GPUs while others compute)\n",
    "- Complex micro-batching & scheduling\n",
    "- Harder to load balance if layers are uneven in cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 15:39:39,445] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-07 15:39:39,471] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "üîÅ Running batch size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  7.82it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  7.67it/s]\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-07 15:39:42,517] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-07 15:39:42,573] [INFO] [logging.py:128:log_dist] [Rank -1] DeepSpeed info: version=0.16.2, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/.local/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "[rank0]:[W507 15:39:42.046035231 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 403, in _distributed_worker\n    res = benchmark_batch_sizes(\n          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 57, in benchmark_batch_sizes\n    elapsed_s, tokens_generated, metrics, cost = benchmark_llm(\n                                                 ^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 201, in benchmark_llm\n    model = load_sharded_model(model_name, dtype, sharding, world_size, seq_len, min_new_tokens, ds_config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 144, in load_sharded_model\n    model = deepspeed.init_inference(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/deepspeed/__init__.py\", line 362, in init_inference\n    ds_inference_config = DeepSpeedInferenceConfig(**config_dict)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/deepspeed/runtime/config_utils.py\", line 57, in __init__\n    super().__init__(**data)\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/pydantic/main.py\", line 253, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 2 validation errors for DeepSpeedInferenceConfig\npipeline_parallel\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'enabled': True, 'pp_size': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npp_engine\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mProcessRaisedException\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     10\u001b[39m max_tokens =  seq_len + min_new_tokens\n\u001b[32m     12\u001b[39m ds_config = {\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpipeline_parallel\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens\n\u001b[32m     18\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m results = \u001b[43mmutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_distributed_benchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNousResearch/Nous-Hermes-Llama2-13b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Number of GPUs\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mds_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mds_config\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m mutils.reset_distributed_and_clear_memory()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/environment/src/utils/model_utils.py:436\u001b[39m, in \u001b[36mrun_distributed_benchmark\u001b[39m\u001b[34m(model_name, seq_len, min_new_tokens, batch_sizes, dtype, sharding, world_size, ds_config)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Manager() \u001b[38;5;28;01mas\u001b[39;00m manager:\n\u001b[32m    434\u001b[39m     shared_results = manager.list()\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[43mmp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_distributed_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmin_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m            \u001b[49m\u001b[43msharding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m            \u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m            \u001b[49m\u001b[43mds_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshared_results\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(shared_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:340\u001b[39m, in \u001b[36mspawn\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method)\u001b[39m\n\u001b[32m    334\u001b[39m     msg = (\n\u001b[32m    335\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    336\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    337\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    338\u001b[39m     )\n\u001b[32m    339\u001b[39m     warnings.warn(msg, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspawn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:296\u001b[39m, in \u001b[36mstart_processes\u001b[39m\u001b[34m(fn, args, nprocs, join, daemon, start_method)\u001b[39m\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py:215\u001b[39m, in \u001b[36mProcessContext.join\u001b[39m\u001b[34m(self, timeout, grace_period)\u001b[39m\n\u001b[32m    213\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m % error_index\n\u001b[32m    214\u001b[39m msg += original_trace\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process.pid)\n",
      "\u001b[31mProcessRaisedException\u001b[39m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/torch/multiprocessing/spawn.py\", line 90, in _wrap\n    fn(i, *args)\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 403, in _distributed_worker\n    res = benchmark_batch_sizes(\n          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 57, in benchmark_batch_sizes\n    elapsed_s, tokens_generated, metrics, cost = benchmark_llm(\n                                                 ^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 201, in benchmark_llm\n    model = load_sharded_model(model_name, dtype, sharding, world_size, seq_len, min_new_tokens, ds_config)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/environment/src/utils/model_utils.py\", line 144, in load_sharded_model\n    model = deepspeed.init_inference(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/deepspeed/__init__.py\", line 362, in init_inference\n    ds_inference_config = DeepSpeedInferenceConfig(**config_dict)\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/deepspeed/runtime/config_utils.py\", line 57, in __init__\n    super().__init__(**data)\n  File \"/home/ec2-user/.local/lib/python3.12/site-packages/pydantic/main.py\", line 253, in __init__\n    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 2 validation errors for DeepSpeedInferenceConfig\npipeline_parallel\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'enabled': True, 'pp_size': 2}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\npp_engine\n  Extra inputs are not permitted [type=extra_forbidden, input_value=True, input_type=bool]\n    For further information visit https://errors.pydantic.dev/2.11/v/extra_forbidden\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import importlib\n",
    "import torch\n",
    "import src.utils.model_utils as mutils\n",
    "importlib.reload(mutils)\n",
    "\n",
    "seq_len = 32\n",
    "min_new_tokens = 1\n",
    "world_size = 2\n",
    "max_tokens =  seq_len + min_new_tokens\n",
    "\n",
    "ds_config = {\n",
    "    \"pipeline_parallel\": {\n",
    "        \"enabled\": True,\n",
    "        \"pp_size\": world_size\n",
    "    },\n",
    "    \"max_tokens\": max_tokens\n",
    "}\n",
    "\n",
    "results = mutils.run_distributed_benchmark(\n",
    "    model_name=\"NousResearch/Nous-Hermes-Llama2-13b\",\n",
    "    seq_len=32,\n",
    "    min_new_tokens=1,\n",
    "    batch_sizes=[1],\n",
    "    dtype=torch.bfloat16,\n",
    "    sharding=\"tensor\",\n",
    "    world_size=2, # Number of GPUs\n",
    "    ds_config=ds_config\n",
    ")\n",
    "mutils.reset_distributed_and_clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
